# Munin æ—¥è®°æ¨¡å¼æŠ€æœ¯æ¶æ„ Review æŠ¥å‘Š

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šå¯¹ Munin æ—¥è®°æ¨¡å¼çš„éœ€æ±‚å˜æ›´è¿›è¡Œå…¨é¢çš„æŠ€æœ¯æ¶æ„ Reviewã€‚æ ¸å¿ƒå˜æ›´åŒ…æ‹¬ï¼š**è§¦å‘é€»è¾‘ä»"24å°æ—¶æ— æ–°æ¶ˆæ¯"æ”¹ä¸º"è·¨å¤©è‡ªç„¶æ—¥è¾¹ç•Œè§¦å‘"**ï¼Œä»¥åŠå¼•å…¥æ•°æ®åº“æ”¯æŒå¤šæ•°æ®æºæ‰©å±•ã€‚

**å…³é”®ç»“è®º**ï¼š
- âœ… å½“å‰æ¶æ„é€‚åˆ MVPï¼Œä½†éœ€è¦å¼•å…¥æ•°æ®åº“å±‚æ”¯æŒæ‰©å±•
- âœ… æ¨èé‡‡ç”¨"Event Sourcing + èšåˆ"æ¨¡å¼å¤„ç†æ—¥è®°åˆå¹¶
- âš ï¸ è±†ç“£ API å·²å…³é—­ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†æ–¹æ¡ˆ
- âš ï¸ å¤šæœåŠ¡åè°ƒéœ€è¦ç»Ÿä¸€çš„ä»»åŠ¡è°ƒåº¦ä¸­å¿ƒ

---

## 1. æ¶æ„è®¾è®¡è¯„ä¼°

### 1.1 å½“å‰æ¶æ„åˆ†æ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        å½“å‰æ¶æ„ (å•ä½“)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Telegram Bot                                              â”‚
â”‚        â”‚                                                    â”‚
â”‚        â–¼                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Message    â”‚â”€â”€â”€â–¶â”‚   GitHub    â”‚â”€â”€â”€â–¶â”‚    Issue     â”‚   â”‚
â”‚   â”‚  Handler    â”‚    â”‚   Client    â”‚    â”‚   Storage    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                â”‚             â”‚
â”‚                                                â–¼             â”‚
â”‚                                         GitHub Actions       â”‚
â”‚                                         (Publish to MD)      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¼˜ç‚¹**ï¼š
- ç®€å•ç›´æ¥ï¼Œè¿ç»´æˆæœ¬ä½
- GitHub Issue å¤©ç„¶æ”¯æŒè¯„è®ºå’Œç‰ˆæœ¬å†å²
- æ— éœ€ç»´æŠ¤æ•°æ®åº“

**ç¼ºç‚¹**ï¼š
- çŠ¶æ€ä¸¢å¤±é£é™©ï¼ˆè¿›ç¨‹é‡å¯ä¼šä¸¢å¤±æœªæäº¤çš„æ¶ˆæ¯ç¼“å†²ï¼‰
- æ— æ³•æ”¯æŒè·¨å¤©åˆå¹¶ï¼ˆéœ€è¦æŒä¹…åŒ–çŠ¶æ€ï¼‰
- ä¸æ”¯æŒå¤šæ•°æ®æºèšåˆ
- æ— æ³•é«˜æ•ˆæŸ¥è¯¢å’Œç»Ÿè®¡

### 1.2 æ‰©å±•æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ¨èæ¶æ„ (æ•°æ®åº“é©±åŠ¨)                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                      API Gateway / Bot Layer                     â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚   â”‚  â”‚  Telegram  â”‚  â”‚   Douban   â”‚  â”‚  Readwise  â”‚  â”‚  Strava  â”‚  â”‚   â”‚
â”‚   â”‚  â”‚    Bot     â”‚  â”‚   Spider   â”‚  â”‚   Syncer   â”‚  â”‚  OAuth   â”‚  â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                     â”‚
â”‚                                    â–¼                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                      Message Queue (Redis/RabbitMQ)              â”‚   â”‚
â”‚   â”‚              å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—ï¼Œæ”¯æŒé‡è¯•å’Œå¹¶å‘æ§åˆ¶                      â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                     â”‚
â”‚                                    â–¼                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                      Core Service Layer                          â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚   â”‚  â”‚  Journal   â”‚  â”‚   Entry    â”‚  â”‚    Sync    â”‚  â”‚  AI      â”‚  â”‚   â”‚
â”‚   â”‚  â”‚  Manager   â”‚  â”‚  Collector â”‚  â”‚ Scheduler  â”‚  â”‚ Summary  â”‚  â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                     â”‚
â”‚                                    â–¼                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                      Data Layer                                  â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚   â”‚  â”‚   SQLite/  â”‚  â”‚   File     â”‚  â”‚   Cache    â”‚  â”‚  Vector  â”‚  â”‚   â”‚
â”‚   â”‚  â”‚  Postgres  â”‚  â”‚  Storage   â”‚  â”‚   (Redis)  â”‚  â”‚   Store  â”‚  â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                     â”‚
â”‚                                    â–¼                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                      Output Layer                                â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚   â”‚  â”‚   GitHub   â”‚  â”‚   Weekly   â”‚  â”‚    API     â”‚  â”‚ Export   â”‚  â”‚   â”‚
â”‚   â”‚  â”‚   Issue    â”‚  â”‚   Report   â”‚  â”‚  Endpoint  â”‚  â”‚  (PDF)   â”‚  â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 æ˜¯å¦éœ€è¦æœåŠ¡æ‹†åˆ†ï¼Ÿ

| é˜¶æ®µ | æ¶æ„ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **MVP (å½“å‰)** | å•ä½“åº”ç”¨ | å•ä¸€æ•°æ®æºï¼Œå¿«é€ŸéªŒè¯ |
| **Phase 1** | å•ä½“ + æ•°æ®åº“ | å¼•å…¥å¤šæ•°æ®æºï¼Œéœ€è¦æŒä¹…åŒ– |
| **Phase 2** | æ¨¡å—åŒ–å•ä½“ | æ¸…æ™°æ¨¡å—è¾¹ç•Œï¼Œä»£ç å¤ç”¨ |
| **Phase 3** | å¾®æœåŠ¡ | é«˜é¢‘å¹¶å‘ï¼Œå¤šç§Ÿæˆ·éƒ¨ç½² |

**æ¨è**ï¼šé‡‡ç”¨ **æ¨¡å—åŒ–å•ä½“** æ¶æ„ï¼Œé€šè¿‡æ¸…æ™°çš„æ¨¡å—åˆ’åˆ†å®ç°"é€»è¾‘æ‹†åˆ†"è€Œé"ç‰©ç†æ‹†åˆ†"ã€‚

---

## 2. æ•°æ®åº“ Schema è®¾è®¡

### 2.1 æ ¸å¿ƒå®ä½“å…³ç³»å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           æ•°æ®æ¨¡å‹å…³ç³»å›¾                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚     Users       â”‚         â”‚    Journals     â”‚         â”‚    Entries     â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ id (PK)         â”‚â—€â”€â”€â”€â”    â”‚ id (PK)         â”‚â—€â”€â”€â”€â”€â”   â”‚ id (PK)        â”‚ â”‚
â”‚  â”‚ telegram_id     â”‚    â”‚    â”‚ user_id (FK)    â”‚â”€â”€â”€â”€â”€â”˜   â”‚ journal_id(FK) â”‚ â”‚
â”‚  â”‚ github_username â”‚    â”‚    â”‚ date            â”‚         â”‚ source_type    â”‚ â”‚
â”‚  â”‚ created_at      â”‚    â”‚    â”‚ status          â”‚         â”‚ source_id      â”‚ â”‚
â”‚  â”‚ settings        â”‚    â”‚    â”‚ merged_at       â”‚         â”‚ content        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚ github_issue_no â”‚         â”‚ metadata       â”‚ â”‚
â”‚                         â”‚    â”‚ ai_summary      â”‚         â”‚ created_at     â”‚ â”‚
â”‚                         â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ updated_at     â”‚ â”‚
â”‚                         â”‚                               â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                         â”‚         1:N                           â”‚          â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Sync Tasks     â”‚         â”‚  Sync Configs   â”‚         â”‚    Media       â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ id (PK)         â”‚         â”‚ id (PK)         â”‚         â”‚ id (PK)        â”‚ â”‚
â”‚  â”‚ user_id (FK)    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ user_id (FK)    â”‚         â”‚ entry_id (FK)  â”‚ â”‚
â”‚  â”‚ source_type     â”‚         â”‚ source_type     â”‚         â”‚ file_path      â”‚ â”‚
â”‚  â”‚ status          â”‚         â”‚ credentials     â”‚         â”‚ file_type      â”‚ â”‚
â”‚  â”‚ last_sync_at    â”‚         â”‚ sync_enabled    â”‚         â”‚ file_size      â”‚ â”‚
â”‚  â”‚ next_sync_at    â”‚         â”‚ sync_schedule   â”‚         â”‚ github_url     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 SQLAlchemy æ¨¡å‹å®šä¹‰

```python
# models/base.py
from datetime import datetime
from typing import Optional
from zoneinfo import ZoneInfo

from sqlalchemy import (
    create_engine, Column, Integer, String, Text, DateTime, 
    ForeignKey, Boolean, JSON, Enum, Index
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship, sessionmaker

Base = declarative_base()

# ============================================================================
# æšä¸¾ç±»å‹å®šä¹‰
# ============================================================================

from enum import Enum as PyEnum

class EntrySourceType(str, PyEnum):
    """æ•°æ®æºç±»å‹"""
    TELEGRAM = "telegram"
    DOUBAN = "douban"
    READWISE = "readwise"
    APPLE_HEALTH = "apple_health"
    STRAVA = "strava"
    MANUAL = "manual"

class JournalStatus(str, PyEnum):
    """æ—¥è®°çŠ¶æ€"""
    COLLECTING = "collecting"      # æ”¶é›†ä¸­
    PENDING_MERGE = "pending_merge" # å¾…åˆå¹¶
    MERGED = "merged"              # å·²åˆå¹¶
    PUBLISHED = "published"        # å·²å‘å¸ƒåˆ° GitHub

class EntryContentType(str, PyEnum):
    """æ¡ç›®å†…å®¹ç±»å‹"""
    TEXT = "text"
    IMAGE = "image"
    DOUBAN_MOVIE = "douban_movie"
    DOUBAN_BOOK = "douban_book"
    DOUBAN_MUSIC = "douban_music"
    READWISE_ARTICLE = "readwise_article"
    HEALTH_WORKOUT = "health_workout"
    STRAVA_ACTIVITY = "strava_activity"


# ============================================================================
# æ•°æ®æ¨¡å‹
# ============================================================================

class User(Base):
    """ç”¨æˆ·è¡¨"""
    __tablename__ = "users"
    
    id = Column(Integer, primary_key=True)
    telegram_id = Column(Integer, unique=True, nullable=False, index=True)
    telegram_username = Column(String(255))
    github_username = Column(String(255))
    github_repo = Column(String(255))
    timezone = Column(String(50), default="Asia/Shanghai")
    
    # ç”¨æˆ·è®¾ç½® (JSON å­˜å‚¨ï¼Œçµæ´»æ‰©å±•)
    settings = Column(JSON, default={
        "auto_merge_enabled": True,
        "ai_summary_enabled": True,
        "weekly_report_enabled": True,
        "default_tags": ["journal"]
    })
    
    created_at = Column(DateTime, default=lambda: datetime.now(ZoneInfo("UTC")))
    updated_at = Column(DateTime, default=lambda: datetime.now(ZoneInfo("UTC")), 
                        onupdate=lambda: datetime.now(ZoneInfo("UTC")))
    
    # å…³ç³»
    journals = relationship("Journal", back_populates="user")
    sync_configs = relationship("SyncConfig", back_populates="user")


class Journal(Base):
    """
    æ—¥è®°è¡¨ - æŒ‰è‡ªç„¶æ—¥èšåˆçš„æ—¥è®°
    
    è®¾è®¡è¯´æ˜ï¼š
    - æ¯ç”¨æˆ·æ¯å¤©ä¸€æ¡æ—¥è®°è®°å½•
    - çŠ¶æ€æœº: COLLECTING -> PENDING_MERGE -> MERGED -> PUBLISHED
    - github_issue_no è®°å½•å¯¹åº”çš„ GitHub Issue
    """
    __tablename__ = "journals"
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False, index=True)
    date = Column(DateTime, nullable=False, index=True)  # æ—¥è®°æ—¥æœŸ (æ—¶åŒºç›¸å…³)
    
    # çŠ¶æ€ç®¡ç†
    status = Column(Enum(JournalStatus), default=JournalStatus.COLLECTING)
    
    # GitHub é›†æˆ
    github_issue_no = Column(Integer)
    github_issue_url = Column(String(500))
    
    # AI æ€»ç»“
    ai_summary = Column(Text)  # AI ç”Ÿæˆçš„å½“æ—¥æ€»ç»“
    ai_summary_model = Column(String(50))  # ä½¿ç”¨çš„æ¨¡å‹
    ai_summary_at = Column(DateTime)
    
    # å…ƒæ•°æ®
    entry_count = Column(Integer, default=0)  # æ¡ç›®æ•°é‡
    word_count = Column(Integer, default=0)   # å­—æ•°ç»Ÿè®¡
    
    # åˆå¹¶ç›¸å…³
    merged_at = Column(DateTime)
    merged_by = Column(String(50))  # 'cron', 'manual', 'api'
    
    created_at = Column(DateTime, default=lambda: datetime.now(ZoneInfo("UTC")))
    updated_at = Column(DateTime, default=lambda: datetime.now(ZoneInfo("UTC")), 
                        onupdate=lambda: datetime.now(ZoneInfo("UTC")))
    
    # å…³ç³»
    user = relationship("User", back_populates="journals")
    entries = relationship("Entry", back_populates="journal", order_by="Entry.created_at")
    
    # å¤åˆç´¢å¼•
    __table_args__ = (
        Index('idx_user_date', 'user_id', 'date', unique=True),
        Index('idx_status_merge', 'status', 'date'),
    )


class Entry(Base):
    """
    æ¡ç›®è¡¨ - å•æ¡è®°å½•ï¼ˆæ¥è‡ªå„æ•°æ®æºï¼‰
    
    è®¾è®¡è¯´æ˜ï¼š
    - æ”¯æŒå¤šç§æ•°æ®æºç»Ÿä¸€å­˜å‚¨
    - content å­˜å‚¨ä¸»è¦å†…å®¹
    - metadata å­˜å‚¨æ•°æ®æºç‰¹å®šçš„é¢å¤–ä¿¡æ¯
    """
    __tablename__ = "entries"
    
    id = Column(Integer, primary_key=True)
    journal_id = Column(Integer, ForeignKey("journals.id"), nullable=False, index=True)
    
    # æ•°æ®æ¥æº
    source_type = Column(Enum(EntrySourceType), nullable=False, index=True)
    source_id = Column(String(255), index=True)  # æ•°æ®æºå”¯ä¸€ID (å¦‚ Telegram message_id)
    
    # å†…å®¹ç±»å‹
    content_type = Column(Enum(EntryContentType), nullable=False)
    
    # å†…å®¹
    content = Column(Text)  # çº¯æ–‡æœ¬å†…å®¹æˆ– Markdown
    raw_content = Column(Text)  # åŸå§‹å†…å®¹ï¼ˆç”¨äºè°ƒè¯•å’Œé‡å¤„ç†ï¼‰
    
    # å…ƒæ•°æ® (JSON æ ¼å¼ï¼Œçµæ´»å­˜å‚¨å„æºç‰¹å®šå­—æ®µ)
    metadata = Column(JSON, default={})
    # ç¤ºä¾‹å…ƒæ•°æ®ï¼š
    # Telegram: {"message_id": 123, "chat_id": 456, "caption": "..."}
    # Douban: {"item_id": "12345", "rating": 5, "title": "..."}
    # Readwise: {"article_id": "abc", "url": "...", "highlights": [...]}
    # Strava: {"activity_id": 123, "distance": 5000, "duration": 1800}
    
    # æ ‡ç­¾ (å†—ä½™å­˜å‚¨æ–¹ä¾¿æŸ¥è¯¢)
    tags = Column(JSON, default=[])  # ["è¯»ä¹¦", "è¿åŠ¨"]
    
    # æ’åºæƒé‡ï¼ˆæ§åˆ¶æ¡ç›®åœ¨æ—¥è®°ä¸­çš„æ˜¾ç¤ºé¡ºåºï¼‰
    sort_order = Column(Integer, default=0)
    
    # æ—¶åŒºç›¸å…³
    created_at = Column(DateTime, default=lambda: datetime.now(ZoneInfo("UTC")))
    source_created_at = Column(DateTime, index=True)  # æ•°æ®æºåˆ›å»ºæ—¶é—´
    timezone = Column(String(50), default="Asia/Shanghai")
    
    # å»é‡ç›¸å…³
    content_hash = Column(String(64), index=True)  # SHA256(content) ç”¨äºå¿«é€Ÿå»é‡
    
    # å…³ç³»
    journal = relationship("Journal", back_populates="entries")
    media_files = relationship("MediaFile", back_populates="entry")
    
    # ç´¢å¼•
    __table_args__ = (
        Index('idx_source_unique', 'source_type', 'source_id', unique=True),
        Index('idx_journal_order', 'journal_id', 'sort_order'),
    )


class MediaFile(Base):
    """åª’ä½“æ–‡ä»¶è¡¨ - å›¾ç‰‡ã€éŸ³é¢‘ç­‰"""
    __tablename__ = "media_files"
    
    id = Column(Integer, primary_key=True)
    entry_id = Column(Integer, ForeignKey("entries.id"), nullable=False, index=True)
    
    # æ–‡ä»¶ä¿¡æ¯
    original_filename = Column(String(500))
    stored_filename = Column(String(500))  # æœ¬åœ°æˆ–å¯¹è±¡å­˜å‚¨è·¯å¾„
    file_type = Column(String(50))  # image/jpeg, image/png, etc.
    file_size = Column(Integer)  # bytes
    
    # GitHub å­˜å‚¨ä¿¡æ¯
    github_path = Column(String(500))  # content/images/2024/02/12/xxx.jpg
    github_url = Column(String(500))  # raw.githubusercontent.com/...
    github_sha = Column(String(100))  # GitHub blob sha
    
    # Telegram æºä¿¡æ¯
    telegram_file_id = Column(String(255))
    telegram_file_unique_id = Column(String(255))
    
    # å›¾ç‰‡å…ƒæ•°æ®
    width = Column(Integer)
    height = Column(Integer)
    
    created_at = Column(DateTime, default=lambda: datetime.now(ZoneInfo("UTC")))
    
    # å…³ç³»
    entry = relationship("Entry", back_populates="media_files")


class SyncConfig(Base):
    """
    æ•°æ®æºåŒæ­¥é…ç½®è¡¨
    
    æ¯ä¸ªç”¨æˆ·æ¯ä¸ªæ•°æ®æºä¸€æ¡é…ç½®
    """
    __tablename__ = "sync_configs"
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False)
    
    source_type = Column(Enum(EntrySourceType), nullable=False)
    
    # è®¤è¯ä¿¡æ¯ (åŠ å¯†å­˜å‚¨)
    credentials = Column(JSON)  # {"token": "xxx", "refresh_token": "yyy"}
    
    # åŒæ­¥è®¾ç½®
    sync_enabled = Column(Boolean, default=True)
    sync_schedule = Column(String(50), default="0 */6 * * *")  # cron è¡¨è¾¾å¼
    sync_direction = Column(String(20), default="pull")  # pull, push, bidirectional
    
    # å¢é‡åŒæ­¥æ ‡è®°
    last_sync_cursor = Column(String(500))  # å„æºç‰¹å®šçš„æ¸¸æ ‡
    last_sync_at = Column(DateTime)
    last_sync_status = Column(String(50))  # success, failed, partial
    last_sync_error = Column(Text)
    
    # è‡ªå®šä¹‰é…ç½®
    config = Column(JSON, default={})
    # Douban: {"user_id": "xxx", "sync_types": ["movie", "book", "music"]}
    # Readwise: {"sync_highlights": true, "sync_articles": true}
    # Strava: {"activity_types": ["Run", "Ride"], "sync_private": false}
    
    created_at = Column(DateTime, default=lambda: datetime.now(ZoneInfo("UTC")))
    updated_at = Column(DateTime, default=lambda: datetime.now(ZoneInfo("UTC")), 
                        onupdate=lambda: datetime.now(ZoneInfo("UTC")))
    
    # å…³ç³»
    user = relationship("User", back_populates="sync_configs")
    
    # å”¯ä¸€çº¦æŸï¼šæ¯ä¸ªç”¨æˆ·æ¯ä¸ªæ•°æ®æºä¸€æ¡é…ç½®
    __table_args__ = (
        Index('idx_user_source', 'user_id', 'source_type', unique=True),
    )


class SyncTask(Base):
    """åŒæ­¥ä»»åŠ¡è¡¨ - è®°å½•æ¯æ¬¡åŒæ­¥æ‰§è¡Œ"""
    __tablename__ = "sync_tasks"
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False, index=True)
    sync_config_id = Column(Integer, ForeignKey("sync_configs.id"))
    source_type = Column(Enum(EntrySourceType), nullable=False)
    
    status = Column(String(50), default="pending")  # pending, running, success, failed
    
    # æ‰§è¡Œä¿¡æ¯
    started_at = Column(DateTime)
    completed_at = Column(DateTime)
    
    # åŒæ­¥ç»“æœç»Ÿè®¡
    items_found = Column(Integer, default=0)
    items_added = Column(Integer, default=0)
    items_skipped = Column(Integer, default=0)  # é‡å¤æ•°æ®
    items_failed = Column(Integer, default=0)
    
    # é”™è¯¯ä¿¡æ¯
    error_message = Column(Text)
    
    # è¯¦ç»†æ—¥å¿—
    logs = Column(JSON, default=[])
    
    created_at = Column(DateTime, default=lambda: datetime.now(ZoneInfo("UTC")))


class WeeklyReport(Base):
    """å‘¨æŠ¥è¡¨ - AI ç”Ÿæˆçš„å‘¨æŠ¥"""
    __tablename__ = "weekly_reports"
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False, index=True)
    
    # å‘¨æœŸ
    year = Column(Integer, nullable=False)
    week_number = Column(Integer, nullable=False)  # ISO week number
    start_date = Column(DateTime, nullable=False)
    end_date = Column(DateTime, nullable=False)
    
    # ç»Ÿè®¡
    total_entries = Column(Integer, default=0)
    total_words = Column(Integer, default=0)
    source_breakdown = Column(JSON, default={})  # {"telegram": 10, "douban": 5}
    
    # AI å†…å®¹
    ai_summary = Column(Text)
    ai_highlights = Column(JSON, default=[])  # æœ¬å‘¨äº®ç‚¹
    ai_recommendations = Column(Text)
    
    # GitHub
    github_issue_no = Column(Integer)
    github_issue_url = Column(String(500))
    
    created_at = Column(DateTime, default=lambda: datetime.now(ZoneInfo("UTC")))
    generated_at = Column(DateTime)
    
    __table_args__ = (
        Index('idx_user_week', 'user_id', 'year', 'week_number', unique=True),
    )


# ============================================================================
# æ•°æ®åº“åˆå§‹åŒ–
# ============================================================================

def init_db(db_url: str = "sqlite:///munin.db"):
    """åˆå§‹åŒ–æ•°æ®åº“"""
    engine = create_engine(db_url, echo=False)
    Base.metadata.create_all(engine)
    Session = sessionmaker(bind=engine)
    return Session
```

### 2.3 å…³é”®æŸ¥è¯¢ç¤ºä¾‹

```python
# æŸ¥è¯¢æŸç”¨æˆ·æŸæ—¥çš„æ—¥è®°åŠæ‰€æœ‰æ¡ç›®
journal = session.query(Journal).filter(
    Journal.user_id == user_id,
    func.date(Journal.date) == date(2024, 2, 12)
).options(
    joinedload(Journal.entries).joinedload(Entry.media_files)
).first()

# æŸ¥è¯¢å¾…åˆå¹¶çš„æ—¥è®°ï¼ˆè·¨å¤©åï¼‰
pending_journals = session.query(Journal).filter(
    Journal.status == JournalStatus.COLLECTING,
    func.date(Journal.date) < func.date(func.now())  # æ—¥æœŸå°äºä»Šå¤©
).all()

# å»é‡æŸ¥è¯¢ - æ£€æŸ¥æŸæ¡ç›®æ˜¯å¦å·²å­˜åœ¨
existing = session.query(Entry).filter(
    Entry.source_type == EntrySourceType.DOUBAN,
    Entry.source_id == douban_item_id
).first()

# æŒ‰æ•°æ®æºç»Ÿè®¡
stats = session.query(
    Entry.source_type,
    func.count(Entry.id).label('count')
).filter(
    Entry.created_at >= start_date,
    Entry.created_at < end_date
).group_by(Entry.source_type).all()
```

---

## 3. å¤šæ•°æ®æºé›†æˆæ–¹æ¡ˆ

### 3.1 æ•°æ®æºæ¥å…¥ç­–ç•¥æ€»è§ˆ

| æ•°æ®æº | API çŠ¶æ€ | æ¥å…¥æ–¹å¼ | ä¼˜å…ˆçº§ | éš¾åº¦ |
|--------|----------|----------|--------|------|
| **Telegram** | âœ… å®˜æ–¹ API | python-telegram-bot | P0 | ä½ |
| **Readwise** | âœ… å®˜æ–¹ API | REST API + OAuth | P1 | ä½ |
| **Strava** | âœ… å®˜æ–¹ API | OAuth2 + REST API | P1 | ä¸­ |
| **Douban** | âŒ å·²å…³é—­ | çˆ¬è™« / ç¬¬ä¸‰æ–¹ RSS | P2 | é«˜ |
| **Apple Health** | âš ï¸ å—é™ | å¯¼å‡ºæ–‡ä»¶ / HealthKit | P2 | é«˜ |

### 3.2 è±†ç“£é›†æˆæ–¹æ¡ˆï¼ˆå…³é”®æŒ‘æˆ˜ï¼‰

**ç°çŠ¶åˆ†æ**ï¼š
- è±†ç“£å®˜æ–¹ API äº 2020 å¹´å…³é—­
- å…¬å¼€é¡µé¢éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®å®Œæ•´å†…å®¹
- æœ‰åçˆ¬æœºåˆ¶

**å¯è¡Œæ–¹æ¡ˆå¯¹æ¯”**ï¼š

| æ–¹æ¡ˆ | ä¼˜ç‚¹ | ç¼ºç‚¹ | æ¨èåº¦ |
|------|------|------|--------|
| **RSS è®¢é˜…** | ç¨³å®šã€æ— éœ€çˆ¬è™« | éœ€è¦è±†ä¼´ç­‰ç¬¬ä¸‰æ–¹æœåŠ¡ | â­â­â­â­ |
| **æµè§ˆå™¨æ’ä»¶** | ç›´æ¥ä»é¡µé¢æå– | éœ€è¦ç”¨æˆ·æ‰‹åŠ¨æ“ä½œ | â­â­â­ |
| **Playwright çˆ¬è™«** | æ•°æ®å®Œæ•´ | éœ€è¦ç»´æŠ¤ç™»å½•æ€ï¼Œä¸ç¨³å®š | â­â­ |
| **è±†ç“£åŒæ­¥åŠ©æ‰‹** | å®˜æ–¹æ”¯æŒ | éœ€è¦å®‰è£…ç‹¬ç«‹åº”ç”¨ | â­â­â­â­ |

**æ¨èå®ç°**ï¼š

```python
# adapters/douban/rss_adapter.py
"""
è±†ç“£ RSS é€‚é…å™¨

ä¾èµ–è±†ä¼´(doufen)ç­‰æœåŠ¡ç”Ÿæˆçš„ RSS Feed
ç”¨æˆ·éœ€è¦æä¾› RSS URL
"""

import feedparser
from datetime import datetime
from typing import List, Dict

class DoubanRSSAdapter:
    """è±†ç“£ RSS é€‚é…å™¨"""
    
    SUPPORTED_TYPES = ['movie', 'book', 'music']
    
    def __init__(self, rss_url: str):
        self.rss_url = rss_url
    
    def fetch_recent_items(self, since: datetime = None) -> List[Dict]:
        """è·å–æœ€è¿‘çš„æ ‡è®°"""
        feed = feedparser.parse(self.rss_url)
        
        items = []
        for entry in feed.entries:
            # è§£æ RSS entry
            item = {
                'source_id': entry.id,
                'title': entry.title,
                'link': entry.link,
                'published': datetime(*entry.published_parsed[:6]),
                'content': entry.get('summary', ''),
                'rating': self._extract_rating(entry),
                'item_type': self._detect_type(entry),
                'tags': [tag.term for tag in entry.get('tags', [])]
            }
            
            if since and item['published'] <= since:
                continue
                
            items.append(item)
        
        return items
    
    def _extract_rating(self, entry) -> int:
        """ä»å†…å®¹ä¸­æå–è¯„åˆ†"""
        # RSS ä¸­å¯èƒ½åŒ…å«è¯„åˆ†ä¿¡æ¯
        content = entry.get('summary', '')
        # å®ç°è¯„åˆ†æå–é€»è¾‘
        return 0
    
    def _detect_type(self, entry) -> str:
        """æ£€æµ‹æ¡ç›®ç±»å‹ï¼ˆç”µå½±/å›¾ä¹¦/éŸ³ä¹ï¼‰"""
        # é€šè¿‡é“¾æ¥æˆ–æ ‡ç­¾åˆ¤æ–­
        if '/subject/' in entry.link:
            # è¿›ä¸€æ­¥åˆ¤æ–­æ˜¯å“ªç§ç±»å‹
            pass
        return 'unknown'


# å¤‡é€‰ï¼šæµè§ˆå™¨ä¹¦ç­¾å°å·¥å…· (Bookmarklet)
"""
å¦‚æœ RSS ä¸å¯ç”¨ï¼Œæä¾›æµè§ˆå™¨ä¹¦ç­¾å·¥å…·è®©ç”¨æˆ·ä¸€é”®å‘é€å½“å‰é¡µé¢

javascript:(function(){
    var data = {
        title: document.title,
        url: window.location.href,
        type: window.location.pathname.includes('/movie/') ? 'movie' : 
              window.location.pathname.includes('/book/') ? 'book' : 'music'
    };
    fetch('https://munin-api.example.com/webhook/douban', {
        method: 'POST',
        headers: {'Content-Type': 'application/json'},
        body: JSON.stringify(data)
    });
})();
"""
```

### 3.3 Readwise é›†æˆ

```python
# adapters/readwise/client.py
"""
Readwise å®˜æ–¹ API å®¢æˆ·ç«¯

æ–‡æ¡£: https://readwise.io/api_deets
"""

import requests
from datetime import datetime
from typing import Iterator, Dict, List

class ReadwiseClient:
    """Readwise API å®¢æˆ·ç«¯"""
    
    BASE_URL = "https://readwise.io/api/v2"
    
    def __init__(self, token: str):
        self.token = token
        self.session = requests.Session()
        self.session.headers.update({
            "Authorization": f"Token {token}"
        })
    
    def get_books(self, updated_after: datetime = None) -> Iterator[Dict]:
        """è·å–ä¹¦æ‘˜åˆ—è¡¨"""
        url = f"{self.BASE_URL}/books/"
        params = {}
        if updated_after:
            params['updated__gt'] = updated_after.isoformat()
        
        while url:
            resp = self.session.get(url, params=params)
            resp.raise_for_status()
            data = resp.json()
            
            for book in data['results']:
                yield book
            
            url = data.get('next')
            params = {}  # åç»­è¯·æ±‚ä½¿ç”¨å®Œæ•´ URL
    
    def get_highlights(self, book_id: str = None, updated_after: datetime = None) -> Iterator[Dict]:
        """è·å–é«˜äº®å†…å®¹"""
        url = f"{self.BASE_URL}/highlights/"
        params = {}
        if book_id:
            params['book_id'] = book_id
        if updated_after:
            params['updated__gt'] = updated_after.isoformat()
        
        while url:
            resp = self.session.get(url, params=params)
            resp.raise_for_status()
            data = resp.json()
            
            for highlight in data['results']:
                yield highlight
            
            url = data.get('next')
    
    def sync_to_entries(self, since: datetime = None) -> List[Dict]:
        """
        åŒæ­¥ Readwise æ•°æ®ä¸º Entry æ ¼å¼
        
        ç­–ç•¥ï¼š
        1. æ¯å¤©çš„æ–‡ç« ä½œä¸ºä¸€ä¸ª Entry
        2. å½“å¤©çš„é«˜äº®èšåˆåˆ°å¯¹åº”æ–‡ç« ä¸­
        3. ä¹¦æ‘˜å•ç‹¬æˆ Entry
        """
        entries = []
        
        for book in self.get_books(updated_after=since):
            # è·å–è¯¥ä¹¦çš„é«˜äº®
            highlights = list(self.get_highlights(book_id=book['id'], updated_after=since))
            
            if highlights:
                entry = {
                    'source_type': 'readwise',
                    'source_id': book['id'],
                    'content_type': 'readwise_article',
                    'title': book['title'],
                    'author': book.get('author'),
                    'url': book.get('source_url'),
                    'content': self._format_highlights(highlights),
                    'metadata': {
                        'category': book.get('category'),  # books, articles, tweets, etc.
                        'num_highlights': len(highlights),
                        'last_highlight_at': book.get('last_highlight_at')
                    }
                }
                entries.append(entry)
        
        return entries
    
    def _format_highlights(self, highlights: List[Dict]) -> str:
        """æ ¼å¼åŒ–é«˜äº®å†…å®¹ä¸º Markdown"""
        parts = []
        for h in highlights:
            parts.append(f"> {h['text']}")
            if h.get('note'):
                parts.append(f"> \n> ğŸ’­ {h['note']}")
            parts.append("")
        return "\n".join(parts)


# OAuth æˆæƒæµç¨‹ (é¦–æ¬¡ä½¿ç”¨)
"""
Readwise ä½¿ç”¨ API Token è€Œé OAuth2ï¼Œè·å–æ–¹å¼ï¼š
1. ç”¨æˆ·ç™»å½• https://readwise.io/access_token
2. å¤åˆ¶ Token
3. åœ¨ Munin é…ç½®ä¸­ç²˜è´´
"""
```

### 3.4 Apple Health é›†æˆ

```python
# adapters/apple_health/parser.py
"""
Apple Health æ•°æ®è§£æå™¨

Apple Health ä¸æ”¯æŒç›´æ¥ APIï¼Œéœ€è¦é€šè¿‡ä»¥ä¸‹æ–¹å¼ï¼š
1. ç”¨æˆ·å¯¼å‡º Health æ•°æ®ï¼ˆé€šè¿‡ Health Appï¼‰
2. ä¸Šä¼ åˆ° Munin
3. è§£æ export.xml

æ›¿ä»£æ–¹æ¡ˆï¼šä½¿ç”¨ç¬¬ä¸‰æ–¹åŒæ­¥å·¥å…·å¦‚ Health Auto Export
"""

import xml.etree.ElementTree as ET
from datetime import datetime
from pathlib import Path
from typing import List, Dict
import zipfile

class AppleHealthParser:
    """Apple Health å¯¼å‡ºæ–‡ä»¶è§£æå™¨"""
    
    def __init__(self, export_path: Path):
        self.export_path = export_path
    
    def parse(self) -> List[Dict]:
        """è§£æå¯¼å‡ºæ–‡ä»¶"""
        # export.xml é€šå¸¸åœ¨å¯¼å‡º zip ä¸­
        if self.export_path.suffix == '.zip':
            xml_content = self._extract_xml_from_zip()
        else:
            xml_content = self.export_path.read_text()
        
        root = ET.fromstring(xml_content)
        
        entries = []
        
        # è§£æè¿åŠ¨è®°å½•
        for workout in root.findall('.//Workout'):
            entry = self._parse_workout(workout)
            if entry:
                entries.append(entry)
        
        # è§£æå¥åº·æŒ‡æ ‡
        for record in root.findall('.//Record[@type="HKQuantityTypeIdentifierHeartRate"]'):
            # å¯é€‰æ‹©æ€§è®°å½•å¿ƒç‡æ•°æ®
            pass
        
        return entries
    
    def _parse_workout(self, workout: ET.Element) -> Dict:
        """è§£æå•æ¬¡è¿åŠ¨"""
        workout_type = workout.get('workoutActivityType', '')
        
        # æ˜ å°„ Apple Health ç±»å‹åˆ°æˆ‘ä»¬çš„ç±»å‹
        type_mapping = {
            'HKWorkoutActivityTypeRunning': 'run',
            'HKWorkoutActivityTypeCycling': 'ride',
            'HKWorkoutActivityTypeWalking': 'walk',
            'HKWorkoutActivityTypeSwimming': 'swim',
            'HKWorkoutActivityTypeYoga': 'yoga',
        }
        
        return {
            'source_type': 'apple_health',
            'source_id': workout.get('UUID'),
            'content_type': 'health_workout',
            'activity_type': type_mapping.get(workout_type, 'other'),
            'start_time': datetime.fromisoformat(workout.get('startDate')),
            'end_time': datetime.fromisoformat(workout.get('endDate')),
            'duration': float(workout.get('duration', 0)),  # åˆ†é’Ÿ
            'distance': self._get_workout_stat(workout, 'Distance'),  # å…¬é‡Œ
            'energy_burned': self._get_workout_stat(workout, 'Energy'),  # å¡è·¯é‡Œ
            'metadata': {
                'raw_type': workout_type,
                'source': workout.get('sourceName')
            }
        }
    
    def _get_workout_stat(self, workout: ET.Element, stat_type: str) -> float:
        """è·å–è¿åŠ¨ç»Ÿè®¡æ•°æ®"""
        for stat in workout.findall(f'.//WorkoutStatistics[@type="HKQuantityTypeIdentifier{stat_type}"]'):
            return float(stat.get('sum', 0))
        return 0.0


# adapters/apple_health/auto_export.py
"""
ä½¿ç”¨ Health Auto Export çš„ webhook åŠŸèƒ½å®ç°è‡ªåŠ¨åŒæ­¥

Health Auto Export æ˜¯ä¸€æ¬¾ iOS Appï¼Œå¯ä»¥ï¼š
1. è‡ªåŠ¨ç›‘æ§ Apple Health æ•°æ®å˜åŒ–
2. é€šè¿‡ webhook å‘é€æ•°æ®
3. æ”¯æŒè‡ªå®šä¹‰æ•°æ®æ ¼å¼

é…ç½®æ­¥éª¤ï¼š
1. å®‰è£… Health Auto Export App
2. é…ç½® webhook URL æŒ‡å‘ Munin API
3. é€‰æ‹©è¦åŒæ­¥çš„æ•°æ®ç±»å‹
"""

from fastapi import APIRouter, Request

router = APIRouter()

@router.post("/webhook/apple-health")
async def apple_health_webhook(request: Request):
    """æ¥æ”¶ Health Auto Export æ¨é€çš„æ•°æ®"""
    data = await request.json()
    
    # Health Auto Export å‘é€çš„æ•°æ®æ ¼å¼
    workout = {
        'source_type': 'apple_health',
        'source_id': data.get('uuid'),
        'activity_type': data.get('activity_type'),
        'start_time': data.get('start_time'),
        'duration': data.get('duration'),
        'distance': data.get('distance'),
        'energy_burned': data.get('calories'),
    }
    
    # ä¿å­˜åˆ°æ•°æ®åº“
    await save_entry(workout)
    
    return {"status": "ok"}
```

### 3.5 Strava é›†æˆ

```python
# adapters/strava/client.py
"""
Strava API å®¢æˆ·ç«¯

æ–‡æ¡£: https://developers.strava.com/docs/
"""

import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional

class StravaClient:
    """Strava API å®¢æˆ·ç«¯"""
    
    BASE_URL = "https://www.strava.com/api/v3"
    
    def __init__(self, access_token: str, refresh_token: str = None, 
                 client_id: str = None, client_secret: str = None):
        self.access_token = access_token
        self.refresh_token = refresh_token
        self.client_id = client_id
        self.client_secret = client_secret
        self.session = requests.Session()
        self._update_auth_header()
    
    def _update_auth_header(self):
        self.session.headers.update({
            "Authorization": f"Bearer {self.access_token}"
        })
    
    def refresh_access_token(self) -> bool:
        """åˆ·æ–° access token"""
        if not all([self.refresh_token, self.client_id, self.client_secret]):
            return False
        
        resp = requests.post(
            "https://www.strava.com/oauth/token",
            data={
                "client_id": self.client_id,
                "client_secret": self.client_secret,
                "refresh_token": self.refresh_token,
                "grant_type": "refresh_token"
            }
        )
        
        if resp.status_code == 200:
            data = resp.json()
            self.access_token = data['access_token']
            self.refresh_token = data['refresh_token']
            self._update_auth_header()
            return True
        return False
    
    def get_activities(self, after: datetime = None, before: datetime = None,
                       per_page: int = 30) -> List[Dict]:
        """è·å–æ´»åŠ¨åˆ—è¡¨"""
        url = f"{self.BASE_URL}/athlete/activities"
        params = {"per_page": per_page}
        
        if after:
            params['after'] = int(after.timestamp())
        if before:
            params['before'] = int(before.timestamp())
        
        resp = self.session.get(url, params=params)
        
        if resp.status_code == 401 and self.refresh_access_token():
            resp = self.session.get(url, params=params)
        
        resp.raise_for_status()
        return resp.json()
    
    def get_activity(self, activity_id: int) -> Dict:
        """è·å–æ´»åŠ¨è¯¦æƒ…"""
        url = f"{self.BASE_URL}/activities/{activity_id}"
        resp = self.session.get(url)
        resp.raise_for_status()
        return resp.json()
    
    def activity_to_entry(self, activity: Dict) -> Dict:
        """è½¬æ¢ Strava æ´»åŠ¨ä¸º Entry æ ¼å¼"""
        return {
            'source_type': 'strava',
            'source_id': str(activity['id']),
            'content_type': 'strava_activity',
            'title': activity.get('name', 'Untitled Activity'),
            'activity_type': activity.get('sport_type', activity.get('type')),
            'start_time': datetime.fromisoformat(activity['start_date_local']),
            'duration': activity.get('elapsed_time', 0) / 60,  # è½¬ä¸ºåˆ†é’Ÿ
            'distance': (activity.get('distance', 0) / 1000) if activity.get('distance') else None,  # è½¬ä¸ºå…¬é‡Œ
            'elevation_gain': activity.get('total_elevation_gain'),  # ç±³
            'average_speed': activity.get('average_speed'),  # m/s
            'max_speed': activity.get('max_speed'),
            'average_heartrate': activity.get('average_heartrate'),
            'max_heartrate': activity.get('max_heartrate'),
            'calories': activity.get('calories'),
            'description': activity.get('description', ''),
            'metadata': {
                'gear_id': activity.get('gear_id'),
                'polyline': activity.get('map', {}).get('summary_polyline'),
                'has_heartrate': activity.get('has_heartrate'),
                'achievement_count': activity.get('achievement_count'),
                'kudos_count': activity.get('kudos_count'),
            }
        }


# OAuth æˆæƒæµç¨‹
"""
Strava OAuth æµç¨‹ï¼š

1. å¼•å¯¼ç”¨æˆ·è®¿é—®æˆæƒ URL:
   https://www.strava.com/oauth/authorize?
     client_id=YOUR_CLIENT_ID&
     response_type=code&
     redirect_uri=YOUR_REDIRECT_URI&
     approval_prompt=force&
     scope=read,activity:read

2. ç”¨æˆ·æˆæƒåï¼ŒStrava é‡å®šå‘åˆ° callback URL å¹¶é™„å¸¦ code

3. ä½¿ç”¨ code æ¢å– access_token:
   POST https://www.strava.com/oauth/token
   {
     "client_id": "YOUR_CLIENT_ID",
     "client_secret": "YOUR_CLIENT_SECRET",
     "code": "AUTHORIZATION_CODE",
     "grant_type": "authorization_code"
   }

4. è¿”å›åŒ…å« access_token å’Œ refresh_token
"""

# FastAPI OAuth å›è°ƒç«¯ç‚¹ç¤ºä¾‹
@router.get("/oauth/strava/callback")
async def strava_oauth_callback(code: str, state: str = None):
    """å¤„ç† Strava OAuth å›è°ƒ"""
    # éªŒè¯ state é˜²æ­¢ CSRF
    
    # æ¢å– token
    resp = requests.post(
        "https://www.strava.com/oauth/token",
        data={
            "client_id": settings.STRAVA_CLIENT_ID,
            "client_secret": settings.STRAVA_CLIENT_SECRET,
            "code": code,
            "grant_type": "authorization_code"
        }
    )
    
    data = resp.json()
    
    # ä¿å­˜åˆ°ç”¨æˆ·é…ç½®
    await save_sync_config(
        user_id=state,
        source_type='strava',
        credentials={
            'access_token': data['access_token'],
            'refresh_token': data['refresh_token'],
            'expires_at': data['expires_at']
        }
    )
    
    return {"status": "success", "message": "Strava æˆæƒæˆåŠŸ"}
```

---

## 4. åŒæ­¥æœºåˆ¶è®¾è®¡

### 4.1 å®šæ—¶ä»»åŠ¡ vs Webhook ç­–ç•¥

| æ•°æ®æº | æ¨èæ–¹å¼ | åŸå›  | é¢‘ç‡ |
|--------|----------|------|------|
| Telegram | Webhook/Long Polling | å®æ—¶æ€§è¦æ±‚é«˜ | å®æ—¶ |
| Readwise | å®šæ—¶ä»»åŠ¡ | API è°ƒç”¨æœ‰é¢‘ç‡é™åˆ¶ | æ¯6å°æ—¶ |
| Strava | å®šæ—¶ä»»åŠ¡ + Webhook | æ”¯æŒ webhook ä½†éœ€é…ç½® | æ¯3å°æ—¶ + å®æ—¶ |
| Apple Health | Webhook (Health Auto Export) | æ— å®˜æ–¹ API | å®æ—¶ |
| Douban | å®šæ—¶ä»»åŠ¡ | RSS åŒæ­¥ | æ¯12å°æ—¶ |

### 4.2 åŒæ­¥è°ƒåº¦å™¨å®ç°

```python
# scheduler/sync_scheduler.py
"""
ç»Ÿä¸€çš„æ•°æ®åŒæ­¥è°ƒåº¦å™¨

ä½¿ç”¨ APScheduler å®ç°å®šæ—¶ä»»åŠ¡è°ƒåº¦
"""

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.events import EVENT_JOB_EXECUTED, EVENT_JOB_ERROR
import asyncio
from datetime import datetime
from typing import Dict, Type

class SyncScheduler:
    """æ•°æ®åŒæ­¥è°ƒåº¦å™¨"""
    
    # æ•°æ®æºé€‚é…å™¨æ˜ å°„
    ADAPTERS: Dict[str, Type] = {
        'readwise': ReadwiseAdapter,
        'strava': StravaAdapter,
        'douban': DoubanRSSAdapter,
    }
    
    def __init__(self, db_session):
        self.db = db_session
        self.scheduler = AsyncIOScheduler()
        self.scheduler.add_listener(
            self._on_job_executed, EVENT_JOB_EXECUTED | EVENT_JOB_ERROR
        )
    
    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        self._load_user_jobs()
        self.scheduler.start()
    
    def _load_user_jobs(self):
        """ä¸ºæ¯ä¸ªå¯ç”¨çš„åŒæ­¥é…ç½®åŠ è½½ä»»åŠ¡"""
        configs = self.db.query(SyncConfig).filter(
            SyncConfig.sync_enabled == True
        ).all()
        
        for config in configs:
            self._schedule_sync(config)
    
    def _schedule_sync(self, config: SyncConfig):
        """è°ƒåº¦å•ä¸ªåŒæ­¥ä»»åŠ¡"""
        job_id = f"sync_{config.user_id}_{config.source_type}"
        
        # å¦‚æœä»»åŠ¡å·²å­˜åœ¨ï¼Œå…ˆç§»é™¤
        if self.scheduler.get_job(job_id):
            self.scheduler.remove_job(job_id)
        
        # ä½¿ç”¨ç”¨æˆ·é…ç½®çš„ cron è¡¨è¾¾å¼
        trigger = CronTrigger.from_crontab(config.sync_schedule)
        
        self.scheduler.add_job(
            func=self._run_sync,
            trigger=trigger,
            id=job_id,
            args=[config.user_id, config.source_type],
            replace_existing=True,
            misfire_grace_time=3600  # 1å°æ—¶å®¹é”™
        )
    
    async def _run_sync(self, user_id: int, source_type: str):
        """æ‰§è¡ŒåŒæ­¥ä»»åŠ¡"""
        # åˆ›å»ºä»»åŠ¡è®°å½•
        task = SyncTask(
            user_id=user_id,
            source_type=source_type,
            status='running',
            started_at=datetime.utcnow()
        )
        self.db.add(task)
        self.db.commit()
        
        try:
            # è·å–é€‚é…å™¨
            adapter_class = self.ADAPTERS.get(source_type)
            if not adapter_class:
                raise ValueError(f"Unknown source type: {source_type}")
            
            # åŠ è½½ç”¨æˆ·é…ç½®
            config = self.db.query(SyncConfig).filter(
                SyncConfig.user_id == user_id,
                SyncConfig.source_type == source_type
            ).first()
            
            adapter = adapter_class(**config.credentials)
            
            # æ‰§è¡ŒåŒæ­¥
            since = config.last_sync_at
            entries = await adapter.sync(since)
            
            # ä¿å­˜æ¡ç›®
            added = 0
            skipped = 0
            for entry_data in entries:
                # å»é‡æ£€æŸ¥
                existing = self.db.query(Entry).filter(
                    Entry.source_type == source_type,
                    Entry.source_id == entry_data['source_id']
                ).first()
                
                if existing:
                    skipped += 1
                    continue
                
                # åˆ›å»º Entry
                entry = Entry(**entry_data, user_id=user_id)
                self.db.add(entry)
                added += 1
            
            # æ›´æ–°é…ç½®çŠ¶æ€
            config.last_sync_at = datetime.utcnow()
            config.last_sync_status = 'success'
            
            # æ›´æ–°ä»»åŠ¡è®°å½•
            task.status = 'success'
            task.items_added = added
            task.items_skipped = skipped
            task.completed_at = datetime.utcnow()
            
            self.db.commit()
            
        except Exception as e:
            # è®°å½•é”™è¯¯
            task.status = 'failed'
            task.error_message = str(e)
            task.completed_at = datetime.utcnow()
            
            config.last_sync_status = 'failed'
            config.last_sync_error = str(e)
            
            self.db.commit()
            raise
    
    def _on_job_executed(self, event):
        """ä»»åŠ¡æ‰§è¡Œå›è°ƒ"""
        if event.exception:
            print(f"Job {event.job_id} failed: {event.exception}")
        else:
            print(f"Job {event.job_id} completed successfully")


# ============================================================================
# è·¨å¤©åˆå¹¶è§¦å‘å™¨ (æ ¸å¿ƒåŠŸèƒ½)
# ============================================================================

class DailyMergeScheduler:
    """
    è·¨å¤©è‡ªåŠ¨åˆå¹¶è°ƒåº¦å™¨
    
    è®¾è®¡è¦ç‚¹ï¼š
    1. æ¯å¤© 00:00 è§¦å‘å‰ä¸€å¤©çš„åˆå¹¶
    2. å¤„ç†æ—¶åŒºé—®é¢˜ - æŒ‰ç”¨æˆ·æ—¶åŒºåˆ¤æ–­æ—¥æœŸ
    3. å¹‚ç­‰æ€§ - é‡å¤æ‰§è¡Œä¸ä¼šé‡å¤åˆ›å»º Issue
    """
    
    def __init__(self, db_session, github_client_factory):
        self.db = db_session
        self.github_factory = github_client_factory
        self.scheduler = AsyncIOScheduler()
    
    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡æ˜¯å¦æœ‰éœ€è¦åˆå¹¶çš„æ—¥è®°
        self.scheduler.add_job(
            self._check_and_merge,
            trigger='cron',
            minute='*/5',  # æ¯5åˆ†é’Ÿæ£€æŸ¥
            id='daily_merge_checker'
        )
        
        self.scheduler.start()
    
    async def _check_and_merge(self):
        """æ£€æŸ¥å¹¶æ‰§è¡Œåˆå¹¶"""
        # è·å–æ‰€æœ‰ç”¨æˆ·
        users = self.db.query(User).all()
        
        for user in users:
            await self._merge_user_pending_journals(user)
    
    async def _merge_user_pending_journals(self, user: User):
        """åˆå¹¶ç”¨æˆ·çš„å¾…å¤„ç†æ—¥è®°"""
        from zoneinfo import ZoneInfo
        
        # è·å–ç”¨æˆ·å½“å‰æ—¶åŒºçš„æ—¥æœŸ
        user_tz = ZoneInfo(user.timezone)
        now_user = datetime.now(user_tz)
        today = now_user.date()
        
        # æŸ¥æ‰¾ COLLECTING çŠ¶æ€ä¸”æ—¥æœŸæ—©äºä»Šå¤©çš„æ—¥è®°
        pending_journals = self.db.query(Journal).filter(
            Journal.user_id == user.id,
            Journal.status == JournalStatus.COLLECTING,
            func.date(Journal.date) < today
        ).all()
        
        for journal in pending_journals:
            await self._merge_journal(journal)
    
    async def _merge_journal(self, journal: Journal):
        """åˆå¹¶å•ç¯‡æ—¥è®°"""
        # çŠ¶æ€å˜æ›´ä¸º PENDING_MERGE
        journal.status = JournalStatus.PENDING_MERGE
        self.db.commit()
        
        try:
            # è·å–æ‰€æœ‰æ¡ç›®
            entries = self.db.query(Entry).filter(
                Entry.journal_id == journal.id
            ).order_by(Entry.sort_order, Entry.created_at).all()
            
            if not entries:
                # ç©ºæ—¥è®°ï¼Œç›´æ¥æ ‡è®°ä¸ºå·²åˆå¹¶
                journal.status = JournalStatus.MERGED
                self.db.commit()
                return
            
            # ç”Ÿæˆ Markdown å†…å®¹
            markdown = self._generate_journal_markdown(journal, entries)
            
            # åˆ›å»º GitHub Issue
            github = self.github_factory(journal.user)
            issue = github.create_issue(
                title=journal.date.strftime("%Y%m%d"),
                body=markdown,
                labels=['journal'] + self._extract_all_tags(entries)
            )
            
            # æ›´æ–°æ—¥è®°çŠ¶æ€
            journal.status = JournalStatus.MERGED
            journal.github_issue_no = issue['number']
            journal.github_issue_url = issue['html_url']
            journal.merged_at = datetime.utcnow()
            journal.merged_by = 'cron'
            
            self.db.commit()
            
        except Exception as e:
            # å›æ»šçŠ¶æ€
            journal.status = JournalStatus.COLLECTING
            self.db.commit()
            raise
    
    def _generate_journal_markdown(self, journal: Journal, entries: List[Entry]) -> str:
        """ç”Ÿæˆæ—¥è®° Markdown å†…å®¹"""
        parts = []
        
        # æ ‡é¢˜
        parts.append(f"# {journal.date.strftime('%Yå¹´%mæœˆ%dæ—¥')} æ—¥è®°")
        parts.append("")
        
        # æŒ‰æ•°æ®æºåˆ†ç»„
        entries_by_source = {}
        for entry in entries:
            source = entry.source_type
            if source not in entries_by_source:
                entries_by_source[source] = []
            entries_by_source[source].append(entry)
        
        # ç”Ÿæˆå„éƒ¨åˆ†å†…å®¹
        source_order = [
            EntrySourceType.TELEGRAM,
            EntrySourceType.DOUBAN,
            EntrySourceType.READWISE,
            EntrySourceType.STRAVA,
            EntrySourceType.APPLE_HEALTH,
        ]
        
        for source in source_order:
            if source not in entries_by_source:
                continue
            
            source_entries = entries_by_source[source]
            section = self._generate_source_section(source, source_entries)
            parts.append(section)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        parts.append("---")
        parts.append("")
        parts.append("### ğŸ“Š ä»Šæ—¥ç»Ÿè®¡")
        parts.append(f"- æ€»æ¡ç›®: {len(entries)}")
        for source, source_entries in entries_by_source.items():
            parts.append(f"- {source.value}: {len(source_entries)}")
        
        return "\n".join(parts)
    
    def _generate_source_section(self, source: EntrySourceType, entries: List[Entry]) -> str:
        """ç”ŸæˆæŸä¸€æ•°æ®æºçš„ Markdown ç« èŠ‚"""
        source_names = {
            EntrySourceType.TELEGRAM: "ğŸ“ éšæ‰‹è®°",
            EntrySourceType.DOUBAN: "ğŸ¬ å½±éŸ³ä¹¦è®°å½•",
            EntrySourceType.READWISE: "ğŸ“š ä»Šæ—¥é˜…è¯»",
            EntrySourceType.STRAVA: "ğŸƒ è¿åŠ¨è®°å½•",
            EntrySourceType.APPLE_HEALTH: "ğŸ’ª å¥åº·æ•°æ®",
        }
        
        parts = []
        parts.append(f"## {source_names.get(source, source.value)}")
        parts.append("")
        
        for entry in entries:
            if entry.content:
                parts.append(entry.content)
                parts.append("")
        
        return "\n".join(parts)
```

### 4.3 æ•°æ®å»é‡ç­–ç•¥

```python
# deduplication/strategies.py
"""
æ•°æ®å»é‡ç­–ç•¥

ä¸åŒæ•°æ®æºé‡‡ç”¨ä¸åŒçš„å»é‡ç­–ç•¥
"""

import hashlib
from typing import Optional
from abc import ABC, abstractmethod

class DeduplicationStrategy(ABC):
    """å»é‡ç­–ç•¥åŸºç±»"""
    
    @abstractmethod
    def generate_key(self, entry_data: dict) -> str:
        """ç”Ÿæˆå»é‡é”®"""
        pass


class SourceIdStrategy(DeduplicationStrategy):
    """
    åŸºäºæ•°æ®æº ID å»é‡
    é€‚ç”¨äºæœ‰ç¨³å®šå”¯ä¸€ ID çš„æºï¼šReadwise, Strava, Telegram
    """
    def generate_key(self, entry_data: dict) -> str:
        source_type = entry_data.get('source_type')
        source_id = entry_data.get('source_id')
        return f"{source_type}:{source_id}"


class ContentHashStrategy(DeduplicationStrategy):
    """
    åŸºäºå†…å®¹å“ˆå¸Œå»é‡
    é€‚ç”¨äºå†…å®¹ç¡®å®šçš„æºï¼šè±†ç“£ã€æ–‡ç« æ‘˜å½•
    """
    def generate_key(self, entry_data: dict) -> str:
        content = entry_data.get('content', '')
        # æ ‡å‡†åŒ–å†…å®¹åå“ˆå¸Œ
        normalized = self._normalize(content)
        hash_value = hashlib.sha256(normalized.encode()).hexdigest()
        return f"hash:{hash_value[:16]}"
    
    def _normalize(self, content: str) -> str:
        """æ ‡å‡†åŒ–å†…å®¹ï¼ˆå»é™¤ç©ºç™½ã€è½¬å°å†™ç­‰ï¼‰"""
        return ' '.join(content.lower().split())


class FuzzyMatchStrategy(DeduplicationStrategy):
    """
    æ¨¡ç³ŠåŒ¹é…å»é‡
    é€‚ç”¨äºå¯èƒ½æœ‰è½»å¾®å˜åŠ¨çš„å†…å®¹
    """
    def __init__(self, threshold: float = 0.9):
        self.threshold = threshold
    
    def generate_key(self, entry_data: dict) -> str:
        # ç”Ÿæˆ SimHash æˆ– MinHash
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„å®ç°
        return f"fuzzy:{self._simhash(entry_data.get('content', ''))}"
    
    def _simhash(self, content: str) -> str:
        # ç®€åŒ–çš„ SimHash å®ç°
        # å®é™…åº”ä½¿ç”¨ä¸“ä¸šçš„æ¨¡ç³ŠåŒ¹é…åº“
        import hashlib
        return hashlib.md5(content[:100].encode()).hexdigest()[:8]


# æ•°æ®æºå»é‡ç­–ç•¥æ˜ å°„
DEDUP_STRATEGIES = {
    'telegram': SourceIdStrategy(),
    'readwise': SourceIdStrategy(),
    'strava': SourceIdStrategy(),
    'douban': ContentHashStrategy(),
    'apple_health': SourceIdStrategy(),
}


async def check_duplicate(db_session, entry_data: dict) -> Optional[Entry]:
    """
    æ£€æŸ¥æ¡ç›®æ˜¯å¦é‡å¤
    
    ä¼˜å…ˆä½¿ç”¨ source_id ç²¾ç¡®åŒ¹é…ï¼Œ
    å¯¹äºæ²¡æœ‰ source_id çš„ï¼Œä½¿ç”¨å†…å®¹å“ˆå¸Œ
    """
    source_type = entry_data.get('source_type')
    strategy = DEDUP_STRATEGIES.get(source_type, ContentHashStrategy())
    
    # å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
    source_id = entry_data.get('source_id')
    if source_id:
        existing = db_session.query(Entry).filter(
            Entry.source_type == source_type,
            Entry.source_id == source_id
        ).first()
        if existing:
            return existing
    
    # å†å°è¯•å†…å®¹å“ˆå¸Œ
    content_hash = entry_data.get('content_hash')
    if content_hash:
        existing = db_session.query(Entry).filter(
            Entry.source_type == source_type,
            Entry.content_hash == content_hash
        ).first()
        if existing:
            return existing
    
    return None
```

---

## 5. AI æ€»ç»“å®ç°

### 5.1 AI æ€»ç»“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      AI Summary Pipeline                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Trigger   â”‚â”€â”€â”€â–¶â”‚  Aggregate  â”‚â”€â”€â”€â–¶â”‚  Generate       â”‚    â”‚
â”‚  â”‚             â”‚    â”‚   Data      â”‚    â”‚  (LLM API)      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚        â”‚                                          â”‚            â”‚
â”‚        â”‚                                          â–¼            â”‚
â”‚        â”‚                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚        â”‚                               â”‚  Post-process   â”‚     â”‚
â”‚        â”‚                               â”‚  - Extract      â”‚     â”‚
â”‚        â”‚                               â”‚    highlights   â”‚     â”‚
â”‚        â”‚                               â”‚  - Format       â”‚     â”‚
â”‚        â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚        â”‚                                          â”‚            â”‚
â”‚        â–¼                                          â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Schedule   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Store Result   â”‚    â”‚
â”‚  â”‚  Next Run   â”‚                        â”‚  (DB/Issue)     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 å®ç°ä»£ç 

```python
# ai/summary_generator.py
"""
AI æ€»ç»“ç”Ÿæˆå™¨

æ”¯æŒå¤šç§ LLM æä¾›å•†ï¼š
- OpenAI GPT-4
- Anthropic Claude
- Azure OpenAI
- æœ¬åœ°æ¨¡å‹ (Ollama)
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Optional
from datetime import datetime
import json
import openai
from anthropic import Anthropic

class LLMProvider(ABC):
    """LLM æä¾›å•†åŸºç±»"""
    
    @abstractmethod
    async def generate(self, prompt: str, system_prompt: str = None) -> str:
        pass


class OpenAIProvider(LLMProvider):
    """OpenAI æä¾›å•†"""
    
    def __init__(self, api_key: str, model: str = "gpt-4-turbo-preview"):
        self.client = openai.AsyncOpenAI(api_key=api_key)
        self.model = model
    
    async def generate(self, prompt: str, system_prompt: str = None) -> str:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=0.7,
            max_tokens=2000
        )
        
        return response.choices[0].message.content


class AnthropicProvider(LLMProvider):
    """Anthropic Claude æä¾›å•†"""
    
    def __init__(self, api_key: str, model: str = "claude-3-sonnet-20240229"):
        self.client = Anthropic(api_key=api_key)
        self.model = model
    
    async def generate(self, prompt: str, system_prompt: str = None) -> str:
        response = await self.client.messages.create(
            model=self.model,
            max_tokens=2000,
            system=system_prompt,
            messages=[{"role": "user", "content": prompt}]
        )
        
        return response.content[0].text


class SummaryGenerator:
    """æ—¥è®°æ€»ç»“ç”Ÿæˆå™¨"""
    
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸ªäººæ—¥è®°åŠ©æ‰‹ï¼Œæ“…é•¿ä»æ—¥å¸¸è®°å½•ä¸­æç‚¼è¦ç‚¹ã€å‘ç°æ¨¡å¼å’Œæ´å¯Ÿã€‚
ä½ éœ€è¦ä»¥æ¸©æš–ã€ç†è§£çš„è¯­æ°”æ€»ç»“ç”¨æˆ·çš„æ—¥è®°å†…å®¹ï¼Œå¸®åŠ©ç”¨æˆ·å›é¡¾å’Œç†è§£è‡ªå·±çš„ä¸€å¤©ã€‚
è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚"""

    DAILY_PROMPT_TEMPLATE = """è¯·æ€»ç»“ä»¥ä¸‹ {date} çš„æ—¥è®°å†…å®¹ï¼š

{content}

è¯·æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºæ€»ç»“ï¼š
{{
    "overview": "ç”¨2-3å¥è¯æ¦‚æ‹¬ä»Šå¤©çš„ä¸»é¢˜å’Œæƒ…ç»ª",
    "highlights": [
        "ä»Šå¤©æœ€é‡è¦çš„3-5ä¸ªäº®ç‚¹æˆ–äº‹ä»¶"
    ],
    "categories": {{
        "å·¥ä½œ/å­¦ä¹ ": "ç›¸å…³å·¥ä½œæˆ–å­¦ä¹ å†…å®¹çš„æ€»ç»“",
        "ç”Ÿæ´»": "ç”Ÿæ´»çäº‹çš„æ€»ç»“",
        "å¨±ä¹": "å½±è§†ã€é˜…è¯»ã€è¿åŠ¨ç­‰å¨±ä¹æ´»åŠ¨çš„æ€»ç»“",
        "æ€è€ƒ": "é‡è¦çš„æƒ³æ³•ã€æ„Ÿæ‚Ÿæˆ–åæ€"
    }},
    "mood": "æ•´ä½“æƒ…ç»ªåˆ¤æ–­ï¼ˆå¦‚ï¼šç§¯æ/å¹³é™/ç–²æƒ«/å…´å¥‹ç­‰ï¼‰",
    "tomorrow_suggestion": "åŸºäºä»Šå¤©çš„å†…å®¹ï¼Œç»™æ˜å¤©çš„ä¸€ä¸ªå°å»ºè®®"
}}

æ³¨æ„ï¼š
- ä¿æŒå®¢è§‚å’Œæ¸©æš–çš„è¯­æ°”
- çªå‡ºç”¨æˆ·å¯èƒ½å¿½è§†çš„äº®ç‚¹
- ä¸è¦è¿‡åº¦è§£è¯»ï¼ŒåŸºäºæ–‡æœ¬å†…å®¹æ€»ç»“
- è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„ JSON"""

    WEEKLY_PROMPT_TEMPLATE = """è¯·æ€»ç»“ä»¥ä¸‹ {start_date} è‡³ {end_date} çš„å‘¨è®°ï¼š

{daily_summaries}

è¯·æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºå‘¨æŠ¥ï¼š
{{
    "theme": "æœ¬å‘¨ä¸»é¢˜",
    "highlights": ["æœ¬å‘¨3-5ä¸ªé‡è¦äº®ç‚¹"],
    "patterns": ["å‘ç°çš„è¡Œä¸ºæˆ–æƒ…ç»ªæ¨¡å¼"],
    "achievements": ["æœ¬å‘¨æˆå°±"],
    "challenges": ["æœ¬å‘¨æŒ‘æˆ˜æˆ–å›°éš¾"],
    "recommendations": ["ä¸‹å‘¨å»ºè®®"]
}}
"""

    def __init__(self, provider: LLMProvider):
        self.provider = provider
    
    async def generate_daily_summary(
        self, 
        date: datetime, 
        entries: List[Entry]
    ) -> Dict:
        """ç”Ÿæˆå•æ—¥æ€»ç»“"""
        
        # èšåˆå†…å®¹
        content_parts = []
        for i, entry in enumerate(entries, 1):
            source_emoji = {
                'telegram': 'ğŸ“',
                'douban': 'ğŸ¬',
                'readwise': 'ğŸ“š',
                'strava': 'ğŸƒ',
                'apple_health': 'ğŸ’ª'
            }.get(entry.source_type.value, 'ğŸ“„')
            
            content_parts.append(f"{source_emoji} æ¡ç›® {i} ({entry.source_type.value}):")
            content_parts.append(entry.content or "(æ— æ–‡å­—å†…å®¹)")
            content_parts.append("")
        
        prompt = self.DAILY_PROMPT_TEMPLATE.format(
            date=date.strftime("%Yå¹´%mæœˆ%dæ—¥"),
            content="\n".join(content_parts)
        )
        
        # ç”Ÿæˆæ€»ç»“
        response = await self.provider.generate(prompt, self.SYSTEM_PROMPT)
        
        # è§£æ JSON
        try:
            # å°è¯•ç›´æ¥è§£æ
            summary = json.loads(response)
        except json.JSONDecodeError:
            # å°è¯•ä» Markdown ä»£ç å—ä¸­æå–
            summary = self._extract_json_from_markdown(response)
        
        return summary
    
    async def generate_weekly_summary(
        self,
        start_date: datetime,
        end_date: datetime,
        daily_journals: List[Journal]
    ) -> Dict:
        """ç”Ÿæˆå‘¨æŠ¥"""
        
        # èšåˆæ¯æ—¥æ€»ç»“
        daily_parts = []
        for journal in daily_journals:
            if journal.ai_summary:
                daily_parts.append(f"{journal.date.strftime('%mæœˆ%dæ—¥')}:")
                daily_parts.append(journal.ai_summary.get('overview', ''))
                daily_parts.append("")
        
        prompt = self.WEEKLY_PROMPT_TEMPLATE.format(
            start_date=start_date.strftime("%Yå¹´%mæœˆ%dæ—¥"),
            end_date=end_date.strftime("%Yå¹´%mæœˆ%dæ—¥"),
            daily_summaries="\n".join(daily_parts)
        )
        
        response = await self.provider.generate(prompt, self.SYSTEM_PROMPT)
        return self._extract_json_from_markdown(response)
    
    def _extract_json_from_markdown(self, text: str) -> Dict:
        """ä» Markdown ä»£ç å—ä¸­æå– JSON"""
        import re
        
        # åŒ¹é… ```json ... ``` æˆ– ``` ... ```
        patterns = [
            r'```json\s*(\{.*?\})\s*```',
            r'```\s*(\{.*?\})\s*```',
            r'(\{[\s\S]*\})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.DOTALL)
            if match:
                try:
                    return json.loads(match.group(1))
                except json.JSONDecodeError:
                    continue
        
        # å¦‚æœéƒ½æ— æ³•è§£æï¼Œè¿”å›åŸå§‹æ–‡æœ¬
        return {"raw": text}


# ============================================================================
# è§¦å‘æœºåˆ¶
# ============================================================================

class AISummaryScheduler:
    """AI æ€»ç»“è°ƒåº¦å™¨"""
    
    def __init__(self, db_session, summary_generator: SummaryGenerator):
        self.db = db_session
        self.generator = summary_generator
        self.scheduler = AsyncIOScheduler()
    
    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        # æ¯æ—¥æ€»ç»“ï¼šæ¯å¤©å‡Œæ™¨ 0:30 ç”Ÿæˆå‰ä¸€å¤©æ€»ç»“
        self.scheduler.add_job(
            self._generate_daily_summaries,
            trigger='cron',
            hour=0,
            minute=30,
            id='daily_summary'
        )
        
        # å‘¨æŠ¥ï¼šæ¯å‘¨ä¸€æ—©ä¸Š 8:00 ç”Ÿæˆä¸Šå‘¨å‘¨æŠ¥
        self.scheduler.add_job(
            self._generate_weekly_reports,
            trigger='cron',
            day_of_week='mon',
            hour=8,
            minute=0,
            id='weekly_report'
        )
        
        self.scheduler.start()
    
    async def _generate_daily_summaries(self):
        """ç”Ÿæˆæ¯æ—¥æ€»ç»“"""
        yesterday = datetime.now().date() - timedelta(days=1)
        
        # è·å–æ˜¨å¤©å·²åˆå¹¶ä½†æœªç”Ÿæˆæ€»ç»“çš„æ—¥è®°
        journals = self.db.query(Journal).filter(
            Journal.status == JournalStatus.MERGED,
            func.date(Journal.date) == yesterday,
            Journal.ai_summary.is_(None)
        ).all()
        
        for journal in journals:
            try:
                # è·å–æ¡ç›®
                entries = self.db.query(Entry).filter(
                    Entry.journal_id == journal.id
                ).all()
                
                # ç”Ÿæˆæ€»ç»“
                summary = await self.generator.generate_daily_summary(
                    journal.date, entries
                )
                
                # ä¿å­˜åˆ°æ—¥è®°
                journal.ai_summary = summary
                journal.ai_summary_model = "gpt-4-turbo"
                journal.ai_summary_at = datetime.utcnow()
                
                self.db.commit()
                
                # å¦‚æœå¯ç”¨äº† GitHub Issue æ›´æ–°ï¼Œæ›´æ–° Issue å†…å®¹
                if journal.github_issue_no:
                    await self._update_github_issue_with_summary(journal)
                
            except Exception as e:
                print(f"Failed to generate summary for journal {journal.id}: {e}")
    
    async def _update_github_issue_with_summary(self, journal: Journal):
        """æ›´æ–° GitHub Issue æ·»åŠ  AI æ€»ç»“"""
        # åœ¨ Issue å†…å®¹æœ«å°¾æ·»åŠ  AI æ€»ç»“
        summary = journal.ai_summary
        if not summary:
            return
        
        # æ„å»ºæ€»ç»“ Markdown
        summary_md = f"""

---

## ğŸ¤– AI æ€»ç»“

### ä»Šæ—¥æ¦‚è§ˆ
{summary.get('overview', '')}

### äº®ç‚¹
{chr(10).join(['- ' + h for h in summary.get('highlights', [])])}

### æƒ…ç»ª
{summary.get('mood', '')}

### æ˜æ—¥å»ºè®®
{summary.get('tomorrow_suggestion', '')}

*Generated by {journal.ai_summary_model} at {journal.ai_summary_at}*
"""
        
        # æ›´æ–° Issueï¼ˆè¿½åŠ åˆ°åŸæœ‰å†…å®¹ï¼‰
        github = GitHubClient(journal.user)
        
        # è·å–å½“å‰ Issue å†…å®¹
        current_body = github.get_issue_body(journal.github_issue_no)
        
        # è¿½åŠ æ€»ç»“
        new_body = current_body + summary_md
        
        github.update_issue_body(journal.github_issue_no, new_body)
```

---

## 6. GitHub Issue æ ¼å¼æ¼”è¿›

### 6.1 å¤šæºæ•°æ®å±•ç¤ºæ ¼å¼

```markdown
<!-- ç”Ÿæˆçš„æ—¥è®° Issue æ ¼å¼ç¤ºä¾‹ -->

# 2024å¹´02æœˆ12æ—¥ æ—¥è®°

<!-- ä½¿ç”¨ HTML æ³¨é‡Šå­˜å‚¨å…ƒæ•°æ®ï¼Œä¸å½±å“æ¸²æŸ“ -->
<!-- 
metadata: {
    "date": "2024-02-12",
    "entry_count": 5,
    "sources": ["telegram", "douban", "readwise"],
    "ai_summary": true
}
-->

---

## ğŸ“ éšæ‰‹è®°

æ—©æ™¨å»å…¬å›­æ•£æ­¥ï¼Œå¤©æ°”çœŸå¥½ #ç”Ÿæ´» #éšæƒ³

![æ™¨è·‘ç…§ç‰‡](/content/images/2024/02/12/photo_074512_abc123.jpg)

---

## ğŸ¬ å½±éŸ³ä¹¦è®°å½•

### ã€Šæ²™ä¸˜2ã€‹
- è¯„åˆ†: â­â­â­â­â­
- è§‚å½±æ—¶é—´: 2024-02-12 19:30
- æ ‡ç­¾: #ç”µå½± #ç§‘å¹»

è§†å¬ç››å®´ï¼Œæ¯”ç¬¬ä¸€éƒ¨æ›´ç²¾å½©ã€‚èµè¾¾äºšçš„è¡¨æ¼”è®©äººå°è±¡æ·±åˆ»ã€‚

### ã€Šç½®èº«äº‹å†…ã€‹
- è¯„åˆ†: â­â­â­â­
- é˜…è¯»è¿›åº¦: è¯»å®Œ
- æ ‡ç­¾: #è¯»ä¹¦ #ç»æµ

ç†è§£ä¸­å›½æ”¿åºœä¸ç»æµå‘å±•çš„å…¥é—¨å¥½ä¹¦ï¼Œé€šä¿—æ˜“æ‡‚ã€‚

---

## ğŸ“š ä»Šæ—¥é˜…è¯»

### [æ–‡ç« æ ‡é¢˜](https://example.com/article)
- æ¥æº: å¾®ä¿¡å…¬ä¼—å· / åšå®¢ / æ–°é—»
- é˜…è¯»æ—¶é•¿: 5åˆ†é’Ÿ

> ç²¾å½©æ‘˜å½•å†…å®¹...
> 
> ğŸ’­ æˆ‘çš„æƒ³æ³•ï¼šè¿™ä¸ªè§‚ç‚¹å¾ˆæœ‰æ„æ€

---

## ğŸƒ è¿åŠ¨è®°å½•

### å‚æ™šè·‘æ­¥
- ç±»å‹: è·‘æ­¥
- è·ç¦»: 5.23 km
- ç”¨æ—¶: 28:45
- é…é€Ÿ: 5:30 /km
- æ¶ˆè€—: 342 kcal
- å¹³å‡å¿ƒç‡: 152 bpm

<!-- Strava æ´»åŠ¨åµŒå…¥ -->
![Strava](https://strava.com/activities/12345678/embed)

---

## ğŸ’ª å¥åº·æ•°æ®

- æ­¥æ•°: 8,456
- æ´»è·ƒçƒ­é‡: 456 kcal
- é™æ¯å¿ƒç‡: 62 bpm

---

## ğŸ¤– AI æ€»ç»“

### ä»Šæ—¥æ¦‚è§ˆ
ä»Šå¤©æ˜¯å……å®çš„ä¸€å¤©ï¼Œå®Œæˆäº†è¿åŠ¨ç›®æ ‡ï¼Œçœ‹å®Œäº†ä¸¤æœ¬/éƒ¨ç”µå½±ï¼Œä¿æŒäº†è‰¯å¥½çš„é˜…è¯»ä¹ æƒ¯ã€‚

### äº®ç‚¹
- æ™¨è·‘äº«å—äº†å¥½å¤©æ°”
- çœ‹å®ŒæœŸå¾…å·²ä¹…çš„ã€Šæ²™ä¸˜2ã€‹
- å®Œæˆäº†ä¸€æœ¬ç»æµç±»ä¹¦ç±çš„é˜…è¯»

### æƒ…ç»ª
ç§¯æã€å……å®

### æ˜æ—¥å»ºè®®
å¯ä»¥å°è¯•æŠŠä»Šå¤©çš„è¯»ä¹¦å¿ƒå¾—è®°å½•ä¸‹æ¥ï¼Œå½¢æˆä¸€ç¯‡å®Œæ•´çš„è¯»ä¹¦ç¬”è®°ã€‚

*Generated by GPT-4 at 2024-02-13T00:30:00Z*

---

## ğŸ“Š ä»Šæ—¥ç»Ÿè®¡

| æ•°æ®æº | æ¡ç›®æ•° | å æ¯” |
|--------|--------|------|
| Telegram | 2 | 40% |
| è±†ç“£ | 2 | 40% |
| Readwise | 1 | 20% |

**æ€»è®¡: 5 æ¡ç›®ï¼Œçº¦ 1,250 å­—**
```

### 6.2 æ ‡ç­¾ç­–ç•¥

```python
# labels/strategy.py

"""
GitHub Issue æ ‡ç­¾ç­–ç•¥

å±‚çº§ç»“æ„ï¼š
- ç±»å‹æ ‡ç­¾ (type:*)
- æ•°æ®æºæ ‡ç­¾ (source:*)
- å†…å®¹æ ‡ç­¾ (ç”¨æˆ·è‡ªå®šä¹‰)
- çŠ¶æ€æ ‡ç­¾ (status:*)
- å‘¨æœŸæ ‡ç­¾ (period:*)
"""

DEFAULT_LABELS = {
    # ç±»å‹æ ‡ç­¾
    'type': [
        'journal',           # æ—¥è®°æ¡ç›®
        'weekly-report',     # å‘¨æŠ¥
        'monthly-report',    # æœˆæŠ¥
        'summary',           # æ€»ç»“
    ],
    
    # æ•°æ®æºæ ‡ç­¾
    'source': [
        'source:telegram',
        'source:douban',
        'source:readwise',
        'source:strava',
        'source:apple-health',
    ],
    
    # çŠ¶æ€æ ‡ç­¾
    'status': [
        'status:collecting',    # æ”¶é›†ä¸­
        'status:merged',        # å·²åˆå¹¶
        'status:ai-summary',    # å·²ç”Ÿæˆ AI æ€»ç»“
        'status:published',     # å·²å‘å¸ƒåˆ°åšå®¢
    ],
    
    # å‘¨æœŸæ ‡ç­¾ï¼ˆå¯é€‰ï¼Œç”¨äºå¿«é€Ÿç­›é€‰ï¼‰
    'period': [
        '2024',
        '2024-Q1',
        '2024-02',
        'week-07',
    ]
}


def generate_labels(journal: Journal, entries: List[Entry]) -> List[str]:
    """ä¸ºæ—¥è®°ç”Ÿæˆæ ‡ç­¾"""
    labels = ['journal']
    
    # æ·»åŠ æ•°æ®æºæ ‡ç­¾
    sources = {e.source_type.value for e in entries}
    for source in sources:
        labels.append(f'source:{source}')
    
    # æ·»åŠ å†…å®¹æ ‡ç­¾ï¼ˆä»ç”¨æˆ·æ ‡ç­¾ä¸­èšåˆï¼‰
    all_tags = set()
    for entry in entries:
        all_tags.update(entry.tags or [])
    labels.extend(all_tags)
    
    # æ·»åŠ çŠ¶æ€æ ‡ç­¾
    labels.append('status:merged')
    
    # æ·»åŠ å‘¨æœŸæ ‡ç­¾
    labels.append(str(journal.date.year))
    labels.append(f"{journal.date.year}-{journal.date.month:02d}")
    
    return labels
```

---

## 7. éƒ¨ç½²å’Œè¿ç»´

### 7.1 æœåŠ¡æ¶æ„

```yaml
# docker-compose.yml
version: '3.8'

services:
  # ä¸»åº”ç”¨
  munin-api:
    build: .
    container_name: munin-api
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=sqlite:///data/munin.db
      - REDIS_URL=redis://redis:6379
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - GITHUB_TOKEN=${GITHUB_TOKEN}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    depends_on:
      - redis
    networks:
      - munin-network

  # åå°ä»»åŠ¡å¤„ç†å™¨
  munin-worker:
    build: .
    container_name: munin-worker
    command: celery -A tasks worker --loglevel=info
    restart: unless-stopped
    environment:
      - DATABASE_URL=sqlite:///data/munin.db
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    depends_on:
      - redis
      - munin-api
    networks:
      - munin-network

  # å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
  munin-scheduler:
    build: .
    container_name: munin-scheduler
    command: celery -A tasks beat --loglevel=info
    restart: unless-stopped
    environment:
      - DATABASE_URL=sqlite:///data/munin.db
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./data:/app/data
    depends_on:
      - redis
    networks:
      - munin-network

  # Redis (ç¼“å­˜ + æ¶ˆæ¯é˜Ÿåˆ—)
  redis:
    image: redis:7-alpine
    container_name: munin-redis
    restart: unless-stopped
    volumes:
      - redis-data:/data
    networks:
      - munin-network

  # å¯é€‰ï¼šWeb ç®¡ç†ç•Œé¢
  munin-web:
    build: ./web
    container_name: munin-web
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - API_URL=http://munin-api:8000
    depends_on:
      - munin-api
    networks:
      - munin-network

volumes:
  redis-data:

networks:
  munin-network:
    driver: bridge
```

### 7.2 ç›‘æ§å’Œå‘Šè­¦

```python
# monitoring/health_check.py
"""
å¥åº·æ£€æŸ¥å’Œç›‘æ§
"""

from fastapi import FastAPI, Response
from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST
import psutil
import time

app = FastAPI()

# Prometheus æŒ‡æ ‡
journal_entries_total = Counter(
    'munin_journal_entries_total',
    'Total number of journal entries',
    ['source_type']
)

sync_duration = Histogram(
    'munin_sync_duration_seconds',
    'Time spent syncing data',
    ['source_type']
)

merge_failures_total = Counter(
    'munin_merge_failures_total',
    'Total number of merge failures'
)

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    checks = {
        "database": await check_database(),
        "redis": await check_redis(),
        "github_api": await check_github_api(),
        "telegram_bot": await check_telegram_bot(),
    }
    
    healthy = all(checks.values())
    
    return {
        "status": "healthy" if healthy else "unhealthy",
        "checks": checks,
        "timestamp": time.time()
    }

@app.get("/metrics")
async def metrics():
    """Prometheus æŒ‡æ ‡ç«¯ç‚¹"""
    return Response(
        content=generate_latest(),
        media_type=CONTENT_TYPE_LATEST
    )


# å‘Šè­¦è§„åˆ™ (Prometheus AlertManager)
ALERT_RULES = """
groups:
- name: munin-alerts
  rules:
  - alert: MuninBotDown
    expr: up{job="munin-api"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Munin Bot is down"
      description: "Munin API has been down for more than 5 minutes"

  - alert: JournalMergeFailures
    expr: rate(munin_merge_failures_total[5m]) > 0.1
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "High rate of journal merge failures"
      description: "More than 10% of journal merges are failing"

  - alert: SyncLag
    expr: time() - munin_last_sync_timestamp > 3600
    for: 15m
    labels:
      severity: warning
    annotations:
      summary: "Data sync is lagging"
      description: "Last successful sync was more than 1 hour ago"
"""

# å¤‡ä»½è„šæœ¬
BACKUP_SCRIPT = """#!/bin/bash
# backup.sh - æ•°æ®å¤‡ä»½è„šæœ¬

BACKUP_DIR="/backups/munin"
DATE=$(date +%Y%m%d_%H%M%S)

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# å¤‡ä»½æ•°æ®åº“
sqlite3 /app/data/munin.db ".backup '$BACKUP_DIR/munin_$DATE.db'"

# å‹ç¼©å¤‡ä»½
gzip $BACKUP_DIR/munin_$DATE.db

# ä¿ç•™æœ€è¿‘30å¤©çš„å¤‡ä»½
find $BACKUP_DIR -name "munin_*.db.gz" -mtime +30 -delete

# å¯é€‰ï¼šä¸Šä¼ åˆ°äº‘å­˜å‚¨
# aws s3 cp $BACKUP_DIR/munin_$DATE.db.gz s3://my-backup-bucket/munin/

echo "Backup completed: munin_$DATE.db.gz"
"""
```

---

## 8. ä»£ç å®ç°å»ºè®®

### 8.1 é¡¹ç›®ç»“æ„

```
/Users/zhengcc/developer/Munin/
â”œâ”€â”€ bot/                          # Telegram Bot æ ¸å¿ƒ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                   # å…¥å£
â”‚   â”œâ”€â”€ config.py                 # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ handlers.py               # æ¶ˆæ¯å¤„ç†å™¨
â”‚   â””â”€â”€ cli.py                    # CLI å·¥å…·
â”‚
â”œâ”€â”€ core/                         # æ ¸å¿ƒæœåŠ¡
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py                 # SQLAlchemy æ¨¡å‹
â”‚   â”œâ”€â”€ database.py               # æ•°æ®åº“è¿æ¥
â”‚   â”œâ”€â”€ journal_manager.py        # æ—¥è®°ç®¡ç†
â”‚   â”œâ”€â”€ entry_collector.py        # æ¡ç›®æ”¶é›†
â”‚   â””â”€â”€ sync_scheduler.py         # åŒæ­¥è°ƒåº¦
â”‚
â”œâ”€â”€ adapters/                     # æ•°æ®æºé€‚é…å™¨
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                   # é€‚é…å™¨åŸºç±»
â”‚   â”œâ”€â”€ telegram.py               # Telegram é€‚é…å™¨
â”‚   â”œâ”€â”€ douban.py                 # è±†ç“£é€‚é…å™¨
â”‚   â”œâ”€â”€ readwise.py               # Readwise é€‚é…å™¨
â”‚   â”œâ”€â”€ strava.py                 # Strava é€‚é…å™¨
â”‚   â””â”€â”€ apple_health.py           # Apple Health é€‚é…å™¨
â”‚
â”œâ”€â”€ ai/                           # AI ç›¸å…³
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ providers.py              # LLM æä¾›å•†
â”‚   â”œâ”€â”€ summary_generator.py      # æ€»ç»“ç”Ÿæˆ
â”‚   â””â”€â”€ prompts.py                # Prompt æ¨¡æ¿
â”‚
â”œâ”€â”€ github/                       # GitHub é›†æˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ client.py                 # GitHub API å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ issue_formatter.py        # Issue æ ¼å¼åŒ–
â”‚   â””â”€â”€ templates/                # Markdown æ¨¡æ¿
â”‚
â”œâ”€â”€ api/                          # REST API
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                   # FastAPI åº”ç”¨
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ journals.py           # æ—¥è®°ç›¸å…³ API
â”‚   â”‚   â”œâ”€â”€ entries.py            # æ¡ç›®ç›¸å…³ API
â”‚   â”‚   â”œâ”€â”€ sync.py               # åŒæ­¥ç›¸å…³ API
â”‚   â”‚   â””â”€â”€ webhooks.py           # Webhook æ¥æ”¶
â”‚
â”œâ”€â”€ tasks/                        # åå°ä»»åŠ¡ (Celery)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ celery_app.py             # Celery é…ç½®
â”‚   â”œâ”€â”€ sync_tasks.py             # åŒæ­¥ä»»åŠ¡
â”‚   â””â”€â”€ merge_tasks.py            # åˆå¹¶ä»»åŠ¡
â”‚
â”œâ”€â”€ web/                          # Web ç®¡ç†ç•Œé¢ (å¯é€‰)
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ src/
â”‚
â”œâ”€â”€ tests/
â”œâ”€â”€ scripts/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### 8.2 é€‚é…å™¨æ¨¡å¼å®ç°

```python
# adapters/base.py
"""
æ•°æ®æºé€‚é…å™¨åŸºç±»

æ‰€æœ‰æ•°æ®æºé€‚é…å™¨å¿…é¡»ç»§æ‰¿æ­¤ç±»å¹¶å®ç°æŠ½è±¡æ–¹æ³•
"""

from abc import ABC, abstractmethod
from typing import Iterator, Dict, Optional
from datetime import datetime

class DataSourceAdapter(ABC):
    """æ•°æ®æºé€‚é…å™¨åŸºç±»"""
    
    source_type: str = None  # å­ç±»å¿…é¡»å®šä¹‰
    
    @abstractmethod
    async def authenticate(self, credentials: dict) -> bool:
        """éªŒè¯å‡­æ®æ˜¯å¦æœ‰æ•ˆ"""
        pass
    
    @abstractmethod
    async def sync(self, since: Optional[datetime] = None) -> Iterator[Dict]:
        """
        åŒæ­¥æ•°æ®
        
        Args:
            since: åªåŒæ­¥æ­¤æ—¶é—´ä¹‹åçš„æ•°æ®
            
        Yields:
            æ ‡å‡†åŒ–æ ¼å¼çš„æ•°æ®æ¡ç›®
        """
        pass
    
    @abstractmethod
    def normalize(self, raw_data: dict) -> Dict:
        """
        å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        
        æ ‡å‡†æ ¼å¼ï¼š
        {
            'source_type': str,          # æ•°æ®æºç±»å‹
            'source_id': str,            # æ•°æ®æºå”¯ä¸€ID
            'content_type': str,         # å†…å®¹ç±»å‹
            'title': str,                # æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
            'content': str,              # Markdown å†…å®¹
            'url': str,                  # åŸå§‹é“¾æ¥ï¼ˆå¯é€‰ï¼‰
            'created_at': datetime,      # åˆ›å»ºæ—¶é—´
            'tags': List[str],           # æ ‡ç­¾
            'metadata': dict,            # åŸå§‹å…ƒæ•°æ®
        }
        """
        pass
    
    @abstractmethod
    async def test_connection(self) -> Dict:
        """æµ‹è¯•è¿æ¥ï¼Œè¿”å›çŠ¶æ€ä¿¡æ¯"""
        pass


# é€‚é…å™¨æ³¨å†Œè¡¨
_ADAPTER_REGISTRY = {}

def register_adapter(source_type: str, adapter_class: type):
    """æ³¨å†Œé€‚é…å™¨"""
    _ADAPTER_REGISTRY[source_type] = adapter_class

def get_adapter(source_type: str, **kwargs) -> DataSourceAdapter:
    """è·å–é€‚é…å™¨å®ä¾‹"""
    adapter_class = _ADAPTER_REGISTRY.get(source_type)
    if not adapter_class:
        raise ValueError(f"Unknown source type: {source_type}")
    return adapter_class(**kwargs)

# ä½¿ç”¨ç¤ºä¾‹
# register_adapter('readwise', ReadwiseAdapter)
# adapter = get_adapter('readwise', token='xxx')
```

---

## 9. å®æ–½è·¯çº¿å›¾

### Phase 1: åŸºç¡€é‡æ„ï¼ˆ2-3 å‘¨ï¼‰

```
Week 1-2: æ•°æ®åº“å¼•å…¥
â”œâ”€â”€ [ ] è®¾è®¡å¹¶å®ç°æ•°æ®åº“æ¨¡å‹
â”œâ”€â”€ [ ] è¿ç§»ç°æœ‰æ•°æ®ï¼ˆGitHub Issue -> DBï¼‰
â”œâ”€â”€ [ ] é‡æ„ handlers.py ä½¿ç”¨æ•°æ®åº“å­˜å‚¨
â””â”€â”€ [ ] å®ç°è·¨å¤©åˆå¹¶é€»è¾‘

Week 2-3: æ ¸å¿ƒåŠŸèƒ½å®Œå–„
â”œâ”€â”€ [ ] å®ç° DailyMergeScheduler
â”œâ”€â”€ [ ] å®Œå–„æ ‡ç­¾æå–å’Œå­˜å‚¨
â”œâ”€â”€ [ ] å®ç° MediaFile æ¨¡å‹å’Œå›¾ç‰‡ç®¡ç†
â””â”€â”€ [ ] æ·»åŠ å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
```

### Phase 2: æ•°æ®æºæ‰©å±•ï¼ˆ3-4 å‘¨ï¼‰

```
Week 4-5: Readwise é›†æˆ
â”œâ”€â”€ [ ] å®ç° ReadwiseAdapter
â”œâ”€â”€ [ ] OAuth/Token è®¤è¯æµç¨‹
â”œâ”€â”€ [ ] å®šæ—¶åŒæ­¥ä»»åŠ¡
â””â”€â”€ [ ] å†…å®¹æ ¼å¼åŒ–

Week 5-6: Strava é›†æˆ
â”œâ”€â”€ [ ] å®ç° StravaAdapter
â”œâ”€â”€ [ ] OAuth2 æˆæƒæµç¨‹
â”œâ”€â”€ [ ] Webhook æ¥æ”¶ç«¯ç‚¹
â””â”€â”€ [ ] æ´»åŠ¨æ•°æ®å±•ç¤º

Week 6-7: è±†ç“£é›†æˆï¼ˆå¤‡é€‰ï¼‰
â”œâ”€â”€ [ ] è°ƒç ” RSS æ–¹æ¡ˆå¯è¡Œæ€§
â”œâ”€â”€ [ ] å®ç° DoubanRSSAdapter
â””â”€â”€ [ ] æˆ–æä¾›æµè§ˆå™¨ä¹¦ç­¾å·¥å…·

Week 7-8: Apple Health
â”œâ”€â”€ [ ] è®¾è®¡ Health Auto Export é›†æˆæ–¹æ¡ˆ
â”œâ”€â”€ [ ] å®ç° webhook æ¥æ”¶
â””â”€â”€ [ ] å¥åº·æ•°æ®æ ¼å¼åŒ–
```

### Phase 3: AI å’Œé«˜çº§åŠŸèƒ½ï¼ˆ2-3 å‘¨ï¼‰

```
Week 9-10: AI æ€»ç»“
â”œâ”€â”€ [ ] å®ç° SummaryGenerator
â”œâ”€â”€ [ ] æ”¯æŒå¤š LLM æä¾›å•†
â”œâ”€â”€ [ ] æ¯æ—¥æ€»ç»“è‡ªåŠ¨ç”Ÿæˆ
â””â”€â”€ [ ] å‘¨æŠ¥è‡ªåŠ¨ç”Ÿæˆ

Week 10-11: é«˜çº§åŠŸèƒ½
â”œâ”€â”€ [ ] æœç´¢åŠŸèƒ½ï¼ˆå…¨æ–‡æ£€ç´¢ï¼‰
â”œâ”€â”€ [ ] ç»Ÿè®¡å’Œå¯è§†åŒ–
â”œâ”€â”€ [ ] å¯¼å‡ºåŠŸèƒ½ï¼ˆPDF, Markdownï¼‰
â””â”€â”€ [ ] æ•°æ®å¤‡ä»½å’Œæ¢å¤
```

### Phase 4: ä¼˜åŒ–å’Œç¨³å®šï¼ˆæŒç»­ï¼‰

```
Week 12+: ä¼˜åŒ–
â”œâ”€â”€ [ ] æ€§èƒ½ä¼˜åŒ–ï¼ˆæŸ¥è¯¢ä¼˜åŒ–ã€ç¼“å­˜ï¼‰
â”œâ”€â”€ [ ] ç›‘æ§å’Œå‘Šè­¦å®Œå–„
â”œâ”€â”€ [ ] æ–‡æ¡£å®Œå–„
â”œâ”€â”€ [ ] ç¤¾åŒºåé¦ˆå¤„ç†
â””â”€â”€ [ ] æ–°æ•°æ®æºæ¢ç´¢
```

---

## 10. å…³é”®å†³ç­–å»ºè®®

### 10.1 æŠ€æœ¯é€‰å‹å†³ç­–

| å†³ç­–é¡¹ | æ¨èæ–¹æ¡ˆ | ç†ç”± |
|--------|----------|------|
| **æ•°æ®åº“** | SQLite (å•æœº) / PostgreSQL (å¤šç”¨æˆ·) | ç®€å•åœºæ™¯ SQLite è¶³å¤Ÿï¼Œæ‰©å±•æ€§å¥½ |
| **ORM** | SQLAlchemy 2.0 | æˆç†Ÿã€æ–‡æ¡£å®Œå–„ã€ç±»å‹æ”¯æŒå¥½ |
| **ä»»åŠ¡é˜Ÿåˆ—** | Celery + Redis | ç¨³å®šã€ç”Ÿæ€ä¸°å¯Œ |
| **LLM é»˜è®¤** | OpenAI GPT-4 | è´¨é‡é«˜ï¼Œåç»­å¯æ”¯æŒå¤šå‚å•† |
| **è±†ç“£æ–¹æ¡ˆ** | RSS ä¼˜å…ˆ | ç¨³å®šå¯é ï¼Œé™ä½ç»´æŠ¤æˆæœ¬ |
| **éƒ¨ç½²** | Docker Compose | ç®€å•ã€å¯ç»´æŠ¤ |

### 10.2 é£é™©æ§åˆ¶

| é£é™© | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|----------|
| è±†ç“£æ–¹æ¡ˆä¸ç¨³å®š | åŠŸèƒ½ä¸å¯ç”¨ | æä¾›æµè§ˆå™¨ä¹¦ç­¾æ›¿ä»£æ–¹æ¡ˆ |
| LLM API æˆæœ¬ | è¿è¥æˆæœ¬ä¸Šå‡ | æ”¯æŒæœ¬åœ°æ¨¡å‹ (Ollama) |
| æ•°æ®ä¸¢å¤± | ç”¨æˆ·æ•°æ®ä¸å¯æ¢å¤ | å®šæœŸå¤‡ä»½ + GitHub Issue å†—ä½™ |
| GitHub API é™æµ | åŒæ­¥å¤±è´¥ | å®ç°æŒ‡æ•°é€€é¿é‡è¯• |

---

## é™„å½•ï¼šå…³é”®ä»£ç ç‰‡æ®µ

### A. è·¨å¤©åˆ¤æ–­å·¥å…·

```python
from datetime import datetime, date
from zoneinfo import ZoneInfo

def is_same_day(dt1: datetime, dt2: datetime, timezone: str = "Asia/Shanghai") -> bool:
    """åˆ¤æ–­ä¸¤ä¸ªæ—¶é—´æ˜¯å¦åœ¨åŒä¸€ä¸ªè‡ªç„¶æ—¥ï¼ˆæŒ‰æŒ‡å®šæ—¶åŒºï¼‰"""
    tz = ZoneInfo(timezone)
    return dt1.astimezone(tz).date() == dt2.astimezone(tz).date()

def get_user_today(timezone: str = "Asia/Shanghai") -> date:
    """è·å–ç”¨æˆ·å½“å‰æ—¶åŒºçš„ä»Šå¤©æ—¥æœŸ"""
    return datetime.now(ZoneInfo(timezone)).date()

def should_merge_journal(journal_date: date, user_timezone: str) -> bool:
    """åˆ¤æ–­æ—¥è®°æ˜¯å¦åº”è¯¥è¢«åˆå¹¶ï¼ˆæ—¥æœŸæ—©äºä»Šå¤©ï¼‰"""
    today = get_user_today(user_timezone)
    return journal_date < today
```

### B. äº‹åŠ¡å®‰å…¨çš„åˆå¹¶æ“ä½œ

```python
from contextlib import contextmanager
from sqlalchemy.orm import Session

@contextmanager
def journal_merge_lock(db: Session, journal_id: int):
    """
    æ—¥è®°åˆå¹¶é”
    
    é˜²æ­¢å¹¶å‘æƒ…å†µä¸‹é‡å¤åˆå¹¶åŒä¸€ç¯‡æ—¥è®°
    """
    try:
        # ä½¿ç”¨ SELECT FOR UPDATE è·å–è¡Œé”
        journal = db.query(Journal).filter(
            Journal.id == journal_id
        ).with_for_update().first()
        
        if not journal:
            raise ValueError(f"Journal {journal_id} not found")
        
        if journal.status != JournalStatus.COLLECTING:
            raise ValueError(f"Journal {journal_id} is not in COLLECTING state")
        
        # æ›´æ–°çŠ¶æ€ä¸ºåˆå¹¶ä¸­
        journal.status = JournalStatus.PENDING_MERGE
        db.commit()
        
        yield journal
        
        # æˆåŠŸå®Œæˆ
        journal.status = JournalStatus.MERGED
        db.commit()
        
    except Exception:
        # å¤±è´¥å›æ»š
        db.rollback()
        # æ¢å¤çŠ¶æ€
        journal = db.query(Journal).get(journal_id)
        if journal:
            journal.status = JournalStatus.COLLECTING
            db.commit()
        raise
```

---

**æŠ¥å‘Šå®Œæˆæ—¥æœŸ**: 2024-02-12  
**ç‰ˆæœ¬**: v1.0  
**ä½œè€…**: Munin Technical Review Agent
